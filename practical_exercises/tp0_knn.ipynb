{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeWu5jyePaOT"
      },
      "source": [
        "# TP0: Les k plus proches voisins\n",
        "\n",
        "**IFT6390 - Fondements de l'apprentissage machine**\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pierrelux/mlbook/blob/main/exercises/tp0_knn.ipynb)\n",
        "\n",
        "Ce notebook accompagne le [Chapitre 1: Methodes non parametriques](https://pierrelux.github.io/mlbook/drafts/knn).\n",
        "\n",
        "## Objectifs\n",
        "\n",
        "A la fin de ce TP, vous serez en mesure de:\n",
        "- Comprendre l'algorithme des k plus proches voisins (k-ppv)\n",
        "- Observer l'effet du parametre k sur les predictions\n",
        "- Comparer differentes fonctions de distance\n",
        "- Apprecier l'importance de la normalisation des donnees\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFHdXD8gPaOU"
      },
      "source": [
        "## Partie 0: Configuration\n",
        "\n",
        "Executez cette cellule pour importer les bibliotheques necessaires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IQfVzyd-PaOV",
        "outputId": "6b9be70b-5879-42da-c58f-fe2bf99e3adb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration terminee!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Pour de jolis graphiques\n",
        "plt.rcParams['figure.figsize'] = (8, 5)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"Configuration terminee!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW5W1hgsPaOV"
      },
      "source": [
        "## Partie 1: L'algorithme k-ppv\n",
        "\n",
        "L'idee des k plus proches voisins est simple: pour classifier un nouveau point, on regarde les $k$ points d'entrainement les plus proches et on prend un vote majoritaire.\n",
        "\n",
        "### Les donnees\n",
        "\n",
        "Commencons par generer un jeu de donnees de classification 2D."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvQJUOenPaOV"
      },
      "outputs": [],
      "source": [
        "# Generation de donnees de classification\n",
        "np.random.seed(42)\n",
        "n_per_class = 15\n",
        "\n",
        "# Classe 0: groupe autour de (-1, -1)\n",
        "X0 = np.random.randn(n_per_class, 2) * 0.6 + np.array([-1, -1])\n",
        "# Classe 1: groupe autour de (1, 1)\n",
        "X1 = np.random.randn(n_per_class, 2) * 0.6 + np.array([1, 1])\n",
        "\n",
        "X_train = np.vstack([X0, X1])\n",
        "y_train = np.array([0] * n_per_class + [1] * n_per_class)\n",
        "\n",
        "print(f\"Nombre de points: {len(X_train)}\")\n",
        "print(f\"Classes: {np.unique(y_train)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yc9BW2SuPaOW"
      },
      "outputs": [],
      "source": [
        "# Visualisation des donnees\n",
        "plt.figure(figsize=(7, 6))\n",
        "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n",
        "           c='C0', s=60, label='Classe 0', edgecolors='k', linewidths=0.5)\n",
        "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n",
        "           c='C1', s=60, label='Classe 1', edgecolors='k', linewidths=0.5)\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "plt.legend()\n",
        "plt.title('Donnees d\\'entrainement')\n",
        "plt.axis('equal')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY4B1zhKPaOW"
      },
      "source": [
        "### Visualiser les k voisins\n",
        "\n",
        "Pour un point requete, identifions ses $k$ plus proches voisins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efQ9VLcCPaOW"
      },
      "outputs": [],
      "source": [
        "# Point requete\n",
        "x_query = np.array([0.3, 0.2])\n",
        "k = 5\n",
        "\n",
        "# Calcul des distances euclidiennes\n",
        "distances = np.sqrt(np.sum((X_train - x_query)**2, axis=1))\n",
        "\n",
        "# Indices des k plus proches\n",
        "k_nearest_idx = np.argsort(distances)[:k]\n",
        "\n",
        "print(f\"Point requete: {x_query}\")\n",
        "print(f\"\\nLes {k} plus proches voisins:\")\n",
        "for i, idx in enumerate(k_nearest_idx):\n",
        "    print(f\"  Voisin {i+1}: point {idx}, distance = {distances[idx]:.3f}, classe = {y_train[idx]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQiuIuMhPaOW"
      },
      "outputs": [],
      "source": [
        "# Visualisation des k voisins\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "\n",
        "# Points d'entrainement\n",
        "ax.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n",
        "           c='C0', s=60, label='Classe 0', zorder=2)\n",
        "ax.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n",
        "           c='C1', s=60, label='Classe 1', zorder=2)\n",
        "\n",
        "# Lignes vers les k voisins et cercles autour\n",
        "for idx in k_nearest_idx:\n",
        "    ax.plot([x_query[0], X_train[idx, 0]], [x_query[1], X_train[idx, 1]],\n",
        "            'k--', alpha=0.4, linewidth=1, zorder=1)\n",
        "    ax.scatter(X_train[idx, 0], X_train[idx, 1],\n",
        "               s=150, facecolors='none', edgecolors='black', linewidths=2, zorder=3)\n",
        "\n",
        "# Point requete\n",
        "ax.scatter(x_query[0], x_query[1], c='red', s=120, marker='*',\n",
        "           label='Requete', zorder=4)\n",
        "\n",
        "# Compter les votes\n",
        "votes = y_train[k_nearest_idx]\n",
        "n_class0 = np.sum(votes == 0)\n",
        "n_class1 = np.sum(votes == 1)\n",
        "prediction = 0 if n_class0 > n_class1 else 1\n",
        "\n",
        "ax.set_xlabel('$x_1$')\n",
        "ax.set_ylabel('$x_2$')\n",
        "ax.legend(loc='upper left')\n",
        "ax.set_title(f'$k = {k}$: votes = [{n_class0} classe 0, {n_class1} classe 1] -> prediction: classe {prediction}')\n",
        "ax.set_aspect('equal')\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfgBpUp-PaOW"
      },
      "source": [
        "### Exercice: Implementer le vote majoritaire\n",
        "\n",
        "Completez la fonction ci-dessous pour compter les votes et predire la classe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPR3LUxSPaOW"
      },
      "outputs": [],
      "source": [
        "def knn_predict(X_train, y_train, x_query, k):\n",
        "    \"\"\"\n",
        "    Predit la classe d'un point en utilisant les k plus proches voisins.\n",
        "\n",
        "    Parametres:\n",
        "        X_train: array (N, d) - points d'entrainement\n",
        "        y_train: array (N,) - etiquettes (0 ou 1)\n",
        "        x_query: array (d,) - point a classifier\n",
        "        k: int - nombre de voisins\n",
        "\n",
        "    Retourne:\n",
        "        prediction: int (0 ou 1)\n",
        "    \"\"\"\n",
        "    # TODO: Calculer les distances euclidiennes de x_query a tous les points\n",
        "    distances = None  # A completer\n",
        "\n",
        "    # TODO: Trouver les indices des k plus proches voisins\n",
        "    k_nearest_idx = None  # A completer\n",
        "\n",
        "    # TODO: Recuperer les etiquettes des k voisins\n",
        "    k_labels = None  # A completer\n",
        "\n",
        "    # TODO: Vote majoritaire (retourner 0 ou 1)\n",
        "    prediction = None  # A completer\n",
        "\n",
        "    return prediction\n",
        "\n",
        "# Test de votre fonction\n",
        "test_pred = knn_predict(X_train, y_train, x_query, k=5)\n",
        "print(f\"Prediction pour {x_query}: classe {test_pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiG-ls-QPaOW"
      },
      "source": [
        "<details>\n",
        "<summary><b>Solution</b> (cliquez pour afficher)</summary>\n",
        "\n",
        "```python\n",
        "def knn_predict(X_train, y_train, x_query, k):\n",
        "    # Calculer les distances euclidiennes\n",
        "    distances = np.sqrt(np.sum((X_train - x_query)**2, axis=1))\n",
        "    \n",
        "    # Trouver les indices des k plus proches voisins\n",
        "    k_nearest_idx = np.argsort(distances)[:k]\n",
        "    \n",
        "    # Recuperer les etiquettes des k voisins\n",
        "    k_labels = y_train[k_nearest_idx]\n",
        "    \n",
        "    # Vote majoritaire\n",
        "    prediction = 1 if np.sum(k_labels) > k/2 else 0\n",
        "    \n",
        "    return prediction\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWEyVzWQPaOX"
      },
      "source": [
        "---\n",
        "## Partie 2: Effet de k\n",
        "\n",
        "Le parametre $k$ controle la complexite du modele:\n",
        "- **$k=1$**: Frontiere tres irreguliere, erreur d'entrainement = 0\n",
        "- **Grand $k$**: Frontiere plus lisse, tend vers la classe majoritaire\n",
        "\n",
        "Visualisons la frontiere de decision pour differentes valeurs de $k$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Btq1DIJuPaOX"
      },
      "outputs": [],
      "source": [
        "def knn_predict_batch(X_train, y_train, X_test, k):\n",
        "    \"\"\"Predit les classes pour un ensemble de points.\"\"\"\n",
        "    predictions = []\n",
        "    for x in X_test:\n",
        "        distances = np.sqrt(np.sum((X_train - x)**2, axis=1))\n",
        "        k_nearest_idx = np.argsort(distances)[:k]\n",
        "        k_labels = y_train[k_nearest_idx]\n",
        "        predictions.append(1 if np.sum(k_labels) > k/2 else 0)\n",
        "    return np.array(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik9Vw--4PaOX"
      },
      "outputs": [],
      "source": [
        "# Creer une grille pour visualiser la frontiere de decision\n",
        "x_min, x_max = X_train[:, 0].min() - 0.5, X_train[:, 0].max() + 0.5\n",
        "y_min, y_max = X_train[:, 1].min() - 0.5, X_train[:, 1].max() + 0.5\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
        "                     np.linspace(y_min, y_max, 100))\n",
        "X_mesh = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "# Visualiser pour differentes valeurs de k\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4.5))\n",
        "k_values = [1, 5, 15]\n",
        "cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n",
        "\n",
        "for ax, k in zip(axes, k_values):\n",
        "    Z = knn_predict_batch(X_train, y_train, X_mesh, k)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    ax.contourf(xx, yy, Z, alpha=0.4, cmap=cmap_light)\n",
        "    ax.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n",
        "               c='C0', s=40, edgecolors='k', linewidths=0.5)\n",
        "    ax.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n",
        "               c='C1', s=40, edgecolors='k', linewidths=0.5)\n",
        "    ax.set_xlim(x_min, x_max)\n",
        "    ax.set_ylim(y_min, y_max)\n",
        "    ax.set_title(f'$k = {k}$')\n",
        "    ax.set_xlabel('$x_1$')\n",
        "    ax.set_ylabel('$x_2$')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sp0G_FWPaOX"
      },
      "source": [
        "**Question**: Que se passe-t-il avec $k=1$? Et avec $k=N$ (tous les points)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3OuuwKZPaOX"
      },
      "source": [
        "### Compromis biais-variance\n",
        "\n",
        "Tracez l'erreur d'entrainement et de test en fonction de $k$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvUQcWNlPaOX"
      },
      "outputs": [],
      "source": [
        "# Generer des donnees de test\n",
        "np.random.seed(123)\n",
        "X0_test = np.random.randn(50, 2) * 0.6 + np.array([-1, -1])\n",
        "X1_test = np.random.randn(50, 2) * 0.6 + np.array([1, 1])\n",
        "X_test = np.vstack([X0_test, X1_test])\n",
        "y_test = np.array([0] * 50 + [1] * 50)\n",
        "\n",
        "# Calculer les erreurs pour differents k\n",
        "k_values = list(range(1, 30, 2))  # k impair pour eviter les egalites\n",
        "train_errors = []\n",
        "test_errors = []\n",
        "\n",
        "for k in k_values:\n",
        "    y_pred_train = knn_predict_batch(X_train, y_train, X_train, k)\n",
        "    y_pred_test = knn_predict_batch(X_train, y_train, X_test, k)\n",
        "    train_errors.append(np.mean(y_pred_train != y_train))\n",
        "    test_errors.append(np.mean(y_pred_test != y_test))\n",
        "\n",
        "# Tracer\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_values, train_errors, 'o-', label='Entrainement')\n",
        "plt.plot(k_values, test_errors, 's-', label='Test')\n",
        "plt.xlabel('$k$ (nombre de voisins)')\n",
        "plt.ylabel('Taux d\\'erreur')\n",
        "plt.title('Compromis biais-variance')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Meilleur k sur le test: {k_values[np.argmin(test_errors)]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq5R6hfCPaOX"
      },
      "source": [
        "---\n",
        "## Partie 3: Fonctions de distance\n",
        "\n",
        "L'algorithme k-ppv depend de la fonction de distance utilisee. Les plus communes sont:\n",
        "\n",
        "- **Distance euclidienne ($\\ell_2$)**: $d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum_j (x_j - y_j)^2}$\n",
        "- **Distance de Manhattan ($\\ell_1$)**: $d(\\mathbf{x}, \\mathbf{y}) = \\sum_j |x_j - y_j|$\n",
        "- **Distance $\\ell_\\infty$**: $d(\\mathbf{x}, \\mathbf{y}) = \\max_j |x_j - y_j|$\n",
        "\n",
        "### Visualiser les boules unite\n",
        "\n",
        "La forme de la boule unite montre ce que chaque norme considere comme \"equidistant\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmHCRVHBPaOX"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "# Points de reference\n",
        "p1 = (1, 0)\n",
        "p2 = (1/np.sqrt(2), 1/np.sqrt(2))  # ~(0.71, 0.71)\n",
        "\n",
        "# L1 norm (losange)\n",
        "ax = axes[0]\n",
        "t = np.linspace(0, 1, 250)\n",
        "x_l1 = np.concatenate([t, 1-t, -t, -1+t])\n",
        "y_l1 = np.concatenate([1-t, -t, -1+t, t])\n",
        "ax.fill(x_l1, y_l1, alpha=0.3, color='C0')\n",
        "ax.plot(x_l1, y_l1, 'C0-', linewidth=2)\n",
        "\n",
        "ax.scatter([p1[0]], [p1[1]], s=80, c='black', zorder=5)\n",
        "ax.scatter([p2[0]], [p2[1]], s=80, c='red', zorder=5)\n",
        "ax.annotate(f'$(1, 0)$\\n$d=1$', p1, textcoords='offset points',\n",
        "            xytext=(5, 10), fontsize=9)\n",
        "ax.annotate(f'$(0.71, 0.71)$\\n$d=1.42$', p2, textcoords='offset points',\n",
        "            xytext=(5, 10), fontsize=9, color='red')\n",
        "\n",
        "ax.set_title(r'Norme $\\ell_1$ (Manhattan)')\n",
        "ax.set_xlim(-1.5, 1.5)\n",
        "ax.set_ylim(-1.5, 1.5)\n",
        "ax.set_aspect('equal')\n",
        "ax.axhline(0, color='gray', linewidth=0.5)\n",
        "ax.axvline(0, color='gray', linewidth=0.5)\n",
        "ax.set_xlabel('$x_1$')\n",
        "ax.set_ylabel('$x_2$')\n",
        "\n",
        "# L2 norm (cercle)\n",
        "ax = axes[1]\n",
        "theta = np.linspace(0, 2*np.pi, 1000)\n",
        "x_l2 = np.cos(theta)\n",
        "y_l2 = np.sin(theta)\n",
        "ax.fill(x_l2, y_l2, alpha=0.3, color='C1')\n",
        "ax.plot(x_l2, y_l2, 'C1-', linewidth=2)\n",
        "\n",
        "ax.scatter([p1[0]], [p1[1]], s=80, c='black', zorder=5)\n",
        "ax.scatter([p2[0]], [p2[1]], s=80, c='black', zorder=5)\n",
        "ax.annotate(f'$(1, 0)$\\n$d=1$', p1, textcoords='offset points',\n",
        "            xytext=(5, 10), fontsize=9)\n",
        "ax.annotate(f'$(0.71, 0.71)$\\n$d=1$', p2, textcoords='offset points',\n",
        "            xytext=(5, 10), fontsize=9)\n",
        "\n",
        "ax.set_title(r'Norme $\\ell_2$ (Euclidienne)')\n",
        "ax.set_xlim(-1.5, 1.5)\n",
        "ax.set_ylim(-1.5, 1.5)\n",
        "ax.set_aspect('equal')\n",
        "ax.axhline(0, color='gray', linewidth=0.5)\n",
        "ax.axvline(0, color='gray', linewidth=0.5)\n",
        "ax.set_xlabel('$x_1$')\n",
        "ax.set_ylabel('$x_2$')\n",
        "\n",
        "# L-infinity norm (carre)\n",
        "ax = axes[2]\n",
        "x_linf = np.array([1, 1, -1, -1, 1])\n",
        "y_linf = np.array([1, -1, -1, 1, 1])\n",
        "ax.fill(x_linf, y_linf, alpha=0.3, color='C2')\n",
        "ax.plot(x_linf, y_linf, 'C2-', linewidth=2)\n",
        "\n",
        "# Pour L-inf: (1,0), (1,0.5), (1,1) ont tous distance 1\n",
        "p_inf = [(1, 0), (1, 0.5), (1, 1)]\n",
        "for i, p in enumerate(p_inf):\n",
        "    ax.scatter([p[0]], [p[1]], s=80, c='black', zorder=5)\n",
        "ax.annotate('$(1, 0)$\\n$d=1$', p_inf[0], textcoords='offset points',\n",
        "            xytext=(-45, -5), fontsize=9)\n",
        "ax.annotate('$(1, 0.5)$\\n$d=1$', p_inf[1], textcoords='offset points',\n",
        "            xytext=(5, -5), fontsize=9)\n",
        "ax.annotate('$(1, 1)$\\n$d=1$', p_inf[2], textcoords='offset points',\n",
        "            xytext=(5, 5), fontsize=9)\n",
        "\n",
        "ax.set_title(r'Norme $\\ell_\\infty$')\n",
        "ax.set_xlim(-1.5, 1.5)\n",
        "ax.set_ylim(-1.5, 1.5)\n",
        "ax.set_aspect('equal')\n",
        "ax.axhline(0, color='gray', linewidth=0.5)\n",
        "ax.axvline(0, color='gray', linewidth=0.5)\n",
        "ax.set_xlabel('$x_1$')\n",
        "ax.set_ylabel('$x_2$')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4fnJTUJPaOX"
      },
      "source": [
        "### Exercice: Calculer les distances\n",
        "\n",
        "Implementez les trois fonctions de distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrTmJreoPaOX"
      },
      "outputs": [],
      "source": [
        "def distance_l1(x, y):\n",
        "    \"\"\"Distance de Manhattan.\"\"\"\n",
        "    # TODO: Implementer\n",
        "    pass\n",
        "\n",
        "def distance_l2(x, y):\n",
        "    \"\"\"Distance euclidienne.\"\"\"\n",
        "    # TODO: Implementer\n",
        "    pass\n",
        "\n",
        "def distance_linf(x, y):\n",
        "    \"\"\"Distance L-infini.\"\"\"\n",
        "    # TODO: Implementer\n",
        "    pass\n",
        "\n",
        "# Test avec deux points\n",
        "x = np.array([0, 0])\n",
        "y = np.array([3, 4])\n",
        "\n",
        "print(f\"Points: x = {x}, y = {y}\")\n",
        "print(f\"Distance L1: {distance_l1(x, y)}\")     # Attendu: 7\n",
        "print(f\"Distance L2: {distance_l2(x, y)}\")     # Attendu: 5\n",
        "print(f\"Distance L-inf: {distance_linf(x, y)}\") # Attendu: 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS6waU0CPaOX"
      },
      "source": [
        "<details>\n",
        "<summary><b>Solution</b> (cliquez pour afficher)</summary>\n",
        "\n",
        "```python\n",
        "def distance_l1(x, y):\n",
        "    return np.sum(np.abs(x - y))\n",
        "\n",
        "def distance_l2(x, y):\n",
        "    return np.sqrt(np.sum((x - y)**2))\n",
        "\n",
        "def distance_linf(x, y):\n",
        "    return np.max(np.abs(x - y))\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FLQlO1GPaOX"
      },
      "source": [
        "### Comment les voisins changent selon la distance\n",
        "\n",
        "Visualisons comment le choix de la metrique affecte les voisins selectionnes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJWCfjLYPaOX"
      },
      "outputs": [],
      "source": [
        "# Implementer les fonctions de distance (solution)\n",
        "def distance_l1(x, y):\n",
        "    return np.sum(np.abs(x - y))\n",
        "\n",
        "def distance_l2(x, y):\n",
        "    return np.sqrt(np.sum((x - y)**2))\n",
        "\n",
        "def distance_linf(x, y):\n",
        "    return np.max(np.abs(x - y))\n",
        "\n",
        "# Point requete\n",
        "x_q = np.array([0, 0])\n",
        "k = 3\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4.5))\n",
        "metrics = [('$\\\\ell_1$', distance_l1), ('$\\\\ell_2$', distance_l2), ('$\\\\ell_\\infty$', distance_linf)]\n",
        "\n",
        "for ax, (name, dist_func) in zip(axes, metrics):\n",
        "    # Calculer les distances avec cette metrique\n",
        "    distances = np.array([dist_func(x_q, xi) for xi in X_train])\n",
        "    k_idx = np.argsort(distances)[:k]\n",
        "\n",
        "    ax.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n",
        "               c='C0', s=50, alpha=0.5)\n",
        "    ax.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n",
        "               c='C1', s=50, alpha=0.5)\n",
        "\n",
        "    # Mettre en evidence les k voisins\n",
        "    for idx in k_idx:\n",
        "        ax.scatter(X_train[idx, 0], X_train[idx, 1],\n",
        "                   s=150, facecolors='none', edgecolors='green', linewidths=2)\n",
        "\n",
        "    ax.scatter(x_q[0], x_q[1], c='red', s=120, marker='*', zorder=5)\n",
        "    ax.set_title(f'Distance {name}')\n",
        "    ax.set_xlabel('$x_1$')\n",
        "    ax.set_ylabel('$x_2$')\n",
        "    ax.set_aspect('equal')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUeYvBtKPaOX"
      },
      "source": [
        "---\n",
        "## Partie 4: Importance de la normalisation\n",
        "\n",
        "Quand les variables ont des echelles differentes, certaines dominent le calcul de distance.\n",
        "\n",
        "### Exemple: age vs revenu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv7oALY0PaOX"
      },
      "outputs": [],
      "source": [
        "# Donnees: age (20-70) et revenu (20000-200000)\n",
        "np.random.seed(42)\n",
        "n = 30\n",
        "\n",
        "# Generer des donnees realistes\n",
        "ages = np.random.uniform(25, 65, n)\n",
        "revenus = ages * 2000 + np.random.normal(0, 20000, n) + 30000  # Correlation avec l'age\n",
        "\n",
        "X_raw = np.column_stack([ages, revenus])\n",
        "\n",
        "# Etiquettes basees sur une combinaison non-lineaire\n",
        "y_raw = ((ages > 45) | (revenus > 100000)).astype(int)\n",
        "\n",
        "print(f\"Age: min={ages.min():.1f}, max={ages.max():.1f}\")\n",
        "print(f\"Revenu: min={revenus.min():.0f}, max={revenus.max():.0f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dh-u2IiPaOX"
      },
      "outputs": [],
      "source": [
        "# Calculer les distances avant normalisation\n",
        "x1 = np.array([30, 50000])   # Jeune, revenu moyen\n",
        "x2 = np.array([35, 51000])   # Similaire en age, revenu proche\n",
        "x3 = np.array([31, 150000])  # Age proche, revenu tres different\n",
        "\n",
        "d_12 = np.sqrt(np.sum((x1 - x2)**2))\n",
        "d_13 = np.sqrt(np.sum((x1 - x3)**2))\n",
        "\n",
        "print(\"AVANT normalisation:\")\n",
        "print(f\"  Distance x1-x2 (age: +5 ans, revenu: +1000$): {d_12:.1f}\")\n",
        "print(f\"  Distance x1-x3 (age: +1 an, revenu: +100000$): {d_13:.1f}\")\n",
        "print(f\"  Ratio: {d_13/d_12:.1f}x plus loin\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbbgQhhkPaOY"
      },
      "source": [
        "**Observation**: La difference de revenu domine completement! Une difference de 100000$ est vue comme 100x plus importante qu'une difference de 5 ans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLbPkas5PaOY"
      },
      "outputs": [],
      "source": [
        "# Normalisation: centrer et reduire\n",
        "mean_age = ages.mean()\n",
        "std_age = ages.std()\n",
        "mean_rev = revenus.mean()\n",
        "std_rev = revenus.std()\n",
        "\n",
        "def normalize(x):\n",
        "    return np.array([(x[0] - mean_age) / std_age,\n",
        "                     (x[1] - mean_rev) / std_rev])\n",
        "\n",
        "x1_norm = normalize(x1)\n",
        "x2_norm = normalize(x2)\n",
        "x3_norm = normalize(x3)\n",
        "\n",
        "d_12_norm = np.sqrt(np.sum((x1_norm - x2_norm)**2))\n",
        "d_13_norm = np.sqrt(np.sum((x1_norm - x3_norm)**2))\n",
        "\n",
        "print(\"APRES normalisation:\")\n",
        "print(f\"  Distance x1-x2: {d_12_norm:.3f}\")\n",
        "print(f\"  Distance x1-x3: {d_13_norm:.3f}\")\n",
        "print(f\"  Ratio: {d_13_norm/d_12_norm:.1f}x plus loin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvYt_wzgPaOY"
      },
      "outputs": [],
      "source": [
        "# Visualiser l'effet de la normalisation\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Avant normalisation\n",
        "ax = axes[0]\n",
        "ax.scatter(X_raw[y_raw == 0, 0], X_raw[y_raw == 0, 1], c='C0', s=60, label='Classe 0')\n",
        "ax.scatter(X_raw[y_raw == 1, 0], X_raw[y_raw == 1, 1], c='C1', s=60, label='Classe 1')\n",
        "ax.scatter([x1[0]], [x1[1]], c='red', s=150, marker='*', zorder=5, label='Requete')\n",
        "ax.set_xlabel('Age (annees)')\n",
        "ax.set_ylabel('Revenu ($)')\n",
        "ax.set_title('Avant normalisation\\n(le revenu domine)')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Apres normalisation\n",
        "X_norm = np.column_stack([(ages - mean_age) / std_age,\n",
        "                          (revenus - mean_rev) / std_rev])\n",
        "\n",
        "ax = axes[1]\n",
        "ax.scatter(X_norm[y_raw == 0, 0], X_norm[y_raw == 0, 1], c='C0', s=60, label='Classe 0')\n",
        "ax.scatter(X_norm[y_raw == 1, 0], X_norm[y_raw == 1, 1], c='C1', s=60, label='Classe 1')\n",
        "ax.scatter([x1_norm[0]], [x1_norm[1]], c='red', s=150, marker='*', zorder=5, label='Requete')\n",
        "ax.set_xlabel('Age (normalise)')\n",
        "ax.set_ylabel('Revenu (normalise)')\n",
        "ax.set_title('Apres normalisation\\n(echelles comparables)')\n",
        "ax.legend()\n",
        "ax.set_aspect('equal')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTNtx2tiPaOY"
      },
      "source": [
        "**Question**: Quelle variable domine le calcul de distance sans normalisation? Pourquoi est-ce problematique pour k-ppv?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIvHx-zuPaOY"
      },
      "source": [
        "---\n",
        "## Recapitulatif\n",
        "\n",
        "Dans ce TP, vous avez appris:\n",
        "\n",
        "1. **Algorithme k-ppv**: Classifier par vote majoritaire des k voisins les plus proches\n",
        "\n",
        "2. **Effet de k**:\n",
        "   - Petit k = frontiere irreguliere (haute variance)\n",
        "   - Grand k = frontiere lisse (haut biais)\n",
        "   \n",
        "3. **Fonctions de distance**: $\\ell_1$ (Manhattan), $\\ell_2$ (Euclidienne), $\\ell_\\infty$\n",
        "   - La forme de la boule unite definit ce qui est \"equidistant\"\n",
        "\n",
        "4. **Normalisation**: Indispensable quand les variables ont des echelles differentes\n",
        "\n",
        "---\n",
        "\n",
        "**Pour aller plus loin**: [Chapitre complet sur le site du cours](https://pierrelux.github.io/mlbook/drafts/knn)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}