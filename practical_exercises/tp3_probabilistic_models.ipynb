{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3: Generative Probabilistic Models\n",
    "\n",
    "**IFT6390 - Fundamentals of Machine Learning**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pierrelux/mlbook/blob/main/exercises/tp3_probabilistic_models.ipynb)\n",
    "\n",
    "This notebook accompanies [Chapter 6: Generative Probabilistic Models](https://pierrelux.github.io/mlbook/ch6_probabilistic_models).\n",
    "\n",
    "## Objectives\n",
    "\n",
    "By the end of this practical exercise, you will be able to:\n",
    "- Estimate the parameters of a naive Bayes classifier by maximum likelihood\n",
    "- Apply Laplace smoothing to avoid zero probabilities\n",
    "- Implement prediction in log-space for numerical stability\n",
    "- Compute the multivariate Gaussian density\n",
    "- Implement the EM algorithm for a Gaussian mixture model\n",
    "- Compare soft clustering (GMM) and hard clustering (k-means)\n",
    "\n",
    "This practical exercise implements **everything from scratch**, without using scikit-learn for training. Scikit-learn is only used to load data and for a final comparison.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2b130a",
   "metadata": {},
   "source": [
    "## Part 0: Setup\n",
    "\n",
    "Run this cell to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d3d9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For nice plots\n",
    "plt.rcParams['figure.figsize'] = (8, 5)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21db5c2",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Text Classification Data\n",
    "\n",
    "The naive Bayes classifier is especially popular for **text classification**. We use a subset of the **20 Newsgroups** dataset, which contains discussion forum posts grouped by topic.\n",
    "\n",
    "Our task: distinguish posts about **medicine** from those about **electronics**, based on the words they contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aa5062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load two categories\n",
    "categories = ['sci.electronics', 'sci.med']\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                                shuffle=True, random_state=42)\n",
    "\n",
    "# Class names\n",
    "class_names = ['Electronics', 'Medicine']\n",
    "\n",
    "# Convert texts to word count matrix\n",
    "vectorizer = CountVectorizer(max_features=2000, stop_words='english')\n",
    "X_counts = vectorizer.fit_transform(newsgroups.data).toarray()\n",
    "y = newsgroups.target\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Number of documents: {len(y)}\")\n",
    "print(f\"Classes: {newsgroups.target_names}\")\n",
    "print(f\"Count matrix: {X_counts.shape} ({X_counts.shape[0]} documents × {X_counts.shape[1]} words)\")\n",
    "print(f\"\\nExample — first 10 words in vocabulary: {list(vocabulary[:10])}\")\n",
    "print(f\"Corresponding counts (document 0): {X_counts[0, :10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f590e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / test split (80% / 20%)\n",
    "np.random.seed(42)\n",
    "n = len(y)\n",
    "indices = np.random.permutation(n)\n",
    "n_train = int(0.8 * n)\n",
    "\n",
    "train_idx = indices[:n_train]\n",
    "test_idx = indices[n_train:]\n",
    "\n",
    "X_train, y_train = X_counts[train_idx], y[train_idx]\n",
    "X_test, y_test = X_counts[test_idx], y[test_idx]\n",
    "\n",
    "print(f\"Training: {len(X_train)} documents\")\n",
    "print(f\"Test: {len(X_test)} documents\")\n",
    "print(f\"\\nDistribution (training):\")\n",
    "for c in [0, 1]:\n",
    "    print(f\"  {class_names[c]}: {np.sum(y_train == c)} documents ({np.mean(y_train == c):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fe5dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent words per class\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for c, ax in enumerate(axes):\n",
    "    mask = y_train == c\n",
    "    word_counts = X_train[mask].sum(axis=0)\n",
    "    top_indices = np.argsort(word_counts)[-15:]\n",
    "\n",
    "    ax.barh(range(15), word_counts[top_indices], color=f'C{c}', alpha=0.7)\n",
    "    ax.set_yticks(range(15))\n",
    "    ax.set_yticklabels([vocabulary[i] for i in top_indices])\n",
    "    ax.set_xlabel(\"Total number of occurrences\")\n",
    "    ax.set_title(f'{class_names[c]}')\n",
    "\n",
    "plt.suptitle('Most frequent words per class', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51ae6a2",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: The Naive Bayes Classifier\n",
    "\n",
    "The naive Bayes classifier assumes that words appear **independently** of each other, given the class. For text classification with word counts, we use the **multinomial** model:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_c \\left[ \\log \\pi_c + \\sum_{d=1}^{D} x_d \\log \\theta_{dc} \\right]$$\n",
    "\n",
    "where:\n",
    "- $\\pi_c = p(y = c)$ is the prior probability of class $c$\n",
    "- $\\theta_{dc} = p(\\text{word } d \\mid y = c)$ is the probability of word $d$ in class $c$\n",
    "- $x_d$ is the number of occurrences of word $d$ in the document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f11481",
   "metadata": {},
   "source": [
    "### Exercise 1: Estimate class priors ★\n",
    "\n",
    "The first step is to estimate the prior probabilities $\\pi_c$. The maximum likelihood estimator is the frequency of each class:\n",
    "\n",
    "$$\\hat{\\pi}_c = \\frac{N_c}{N}$$\n",
    "\n",
    "We work in log-space for numerical stability. Complete the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef987ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_log_priors(y_train):\n",
    "    \"\"\"\n",
    "    Estimate the log prior probabilities of each class.\n",
    "\n",
    "    Args:\n",
    "        y_train: training labels (N,)\n",
    "\n",
    "    Returns:\n",
    "        log_priors: array of size (C,) containing log(pi_c) for each class\n",
    "    \"\"\"\n",
    "    classes = np.unique(y_train)\n",
    "    N = len(y_train)\n",
    "\n",
    "    # ============================================\n",
    "    # TODO: Compute the log prior probabilities\n",
    "    # Hint: log(N_c / N) for each class c\n",
    "    # ============================================\n",
    "\n",
    "    log_priors = None  # <- Replace with your code\n",
    "\n",
    "    return log_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2061b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function\n",
    "log_priors = estimate_log_priors(y_train)\n",
    "\n",
    "if log_priors is not None:\n",
    "    for c in range(len(log_priors)):\n",
    "        print(f\"log pi({class_names[c]}) = {log_priors[c]:.4f}  ->  pi = {np.exp(log_priors[c]):.4f}\")\n",
    "\n",
    "    # Check: probabilities must sum to 1\n",
    "    if np.isclose(np.sum(np.exp(log_priors)), 1.0):\n",
    "        print(\"\\nCorrect! The probabilities sum to 1.\")\n",
    "    else:\n",
    "        print(\"\\nCheck your implementation.\")\n",
    "else:\n",
    "    print(\"Complete the estimate_log_priors function!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0137fa8",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b> (click to reveal)</summary>\n",
    "\n",
    "```python\n",
    "def estimate_log_priors(y_train):\n",
    "    classes = np.unique(y_train)\n",
    "    N = len(y_train)\n",
    "    log_priors = np.array([np.log(np.sum(y_train == c) / N) for c in classes])\n",
    "    return log_priors\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e98c49",
   "metadata": {},
   "source": [
    "### Exercise 2: Estimate conditional probabilities ★\n",
    "\n",
    "For the multinomial model, the probability of word $d$ in class $c$ is estimated by:\n",
    "\n",
    "$$\\hat{\\theta}_{dc} = \\frac{N_{dc} + \\alpha}{N_c^{\\text{words}} + \\alpha V}$$\n",
    "\n",
    "where:\n",
    "- $N_{dc}$ = total number of occurrences of word $d$ in documents of class $c$\n",
    "- $N_c^{\\text{words}} = \\sum_d N_{dc}$ = total number of words in class $c$\n",
    "- $V$ = vocabulary size\n",
    "- $\\alpha$ = Laplace smoothing parameter ($\\alpha = 1$ by default)\n",
    "\n",
    "Laplace smoothing prevents zero probabilities: without it, a single absent word would be enough to eliminate a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_log_likelihoods(X_train, y_train, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Estimate the log conditional probabilities of each word per class.\n",
    "\n",
    "    Args:\n",
    "        X_train: count matrix (N, D)\n",
    "        y_train: labels (N,)\n",
    "        alpha: Laplace smoothing parameter\n",
    "\n",
    "    Returns:\n",
    "        log_likelihoods: matrix (C, D) where entry [c, d] = log(theta_dc)\n",
    "    \"\"\"\n",
    "    classes = np.unique(y_train)\n",
    "    C = len(classes)\n",
    "    D = X_train.shape[1]\n",
    "    V = D  # vocabulary size\n",
    "\n",
    "    log_likelihoods = np.zeros((C, D))\n",
    "\n",
    "    for c in classes:\n",
    "        # Documents of class c\n",
    "        X_c = X_train[y_train == c]\n",
    "\n",
    "        # ============================================\n",
    "        # TODO: Compute the log conditional probabilities\n",
    "        # 1. N_dc = X_c.sum(axis=0)  (occurrences of each word)\n",
    "        # 2. N_c_words = N_dc.sum()  (total words in the class)\n",
    "        # 3. theta_dc = (N_dc + alpha) / (N_c_words + alpha * V)\n",
    "        # 4. log_likelihoods[c] = np.log(theta_dc)\n",
    "        # ============================================\n",
    "\n",
    "        log_likelihoods[c] = None  # <- Replace with your code\n",
    "\n",
    "    return log_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c57c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function\n",
    "log_lk = estimate_log_likelihoods(X_train, y_train, alpha=1.0)\n",
    "\n",
    "if log_lk is not None and not np.any(log_lk == 0):\n",
    "    print(f\"Matrix shape: {log_lk.shape} (classes × words)\")\n",
    "\n",
    "    for c in range(2):\n",
    "        prob_sum = np.exp(log_lk[c]).sum()\n",
    "        print(f\"\\nClass {class_names[c]}:\")\n",
    "        print(f\"  Sum of probabilities: {prob_sum:.4f} (expected: ≈ 1.0)\")\n",
    "\n",
    "        top5 = np.argsort(log_lk[c])[-5:][::-1]\n",
    "        print(f\"  Top 5 words: {[vocabulary[i] for i in top5]}\")\n",
    "\n",
    "    if all(np.isclose(np.exp(log_lk[c]).sum(), 1.0, atol=1e-5) for c in range(2)):\n",
    "        print(\"\\nCorrect!\")\n",
    "else:\n",
    "    print(\"Complete the estimate_log_likelihoods function!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3bf958",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b> (click to reveal)</summary>\n",
    "\n",
    "```python\n",
    "def estimate_log_likelihoods(X_train, y_train, alpha=1.0):\n",
    "    classes = np.unique(y_train)\n",
    "    C = len(classes)\n",
    "    D = X_train.shape[1]\n",
    "    V = D\n",
    "\n",
    "    log_likelihoods = np.zeros((C, D))\n",
    "\n",
    "    for c in classes:\n",
    "        X_c = X_train[y_train == c]\n",
    "        N_dc = X_c.sum(axis=0)\n",
    "        N_c_words = N_dc.sum()\n",
    "        theta_dc = (N_dc + alpha) / (N_c_words + alpha * V)\n",
    "        log_likelihoods[c] = np.log(theta_dc)\n",
    "\n",
    "    return log_likelihoods\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53963cf3",
   "metadata": {},
   "source": [
    "### Exercise 3: Predict with naive Bayes ★★\n",
    "\n",
    "To classify a document, we compute the score of each class and choose the most probable one:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_c \\left[ \\log \\pi_c + \\sum_{d=1}^{D} x_d \\log \\theta_{dc} \\right]$$\n",
    "\n",
    "In matrix form: $\\text{scores} = \\mathbf{X} \\cdot \\log\\boldsymbol{\\Theta}^\\top + \\log\\boldsymbol{\\pi}$\n",
    "\n",
    "Computing in log-space avoids numerical underflow problems: the product of thousands of probabilities close to zero would yield a result indistinguishable from zero in floating-point arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858ec45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_naive_bayes(X, log_priors, log_likelihoods):\n",
    "    \"\"\"\n",
    "    Predict classes with the naive Bayes classifier.\n",
    "\n",
    "    Args:\n",
    "        X: count matrix (N, D)\n",
    "        log_priors: log prior probabilities (C,)\n",
    "        log_likelihoods: log conditional probabilities (C, D)\n",
    "\n",
    "    Returns:\n",
    "        predictions: predicted classes (N,)\n",
    "        log_scores: score matrix (N, C)\n",
    "    \"\"\"\n",
    "    # ============================================\n",
    "    # TODO: Compute the log scores for each class\n",
    "    # scores = X @ log_likelihoods.T + log_priors\n",
    "    # predictions = argmax of scores per row\n",
    "    # ============================================\n",
    "\n",
    "    log_scores = None  # <- Replace with your code\n",
    "    predictions = None  # <- Replace with your code\n",
    "\n",
    "    return predictions, log_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e1fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function\n",
    "if log_priors is not None and log_lk is not None and not np.any(log_lk == 0):\n",
    "    y_pred, scores = predict_naive_bayes(X_test, log_priors, log_lk)\n",
    "\n",
    "    if y_pred is not None:\n",
    "        accuracy = np.mean(y_pred == y_test)\n",
    "        print(f\"Test accuracy: {accuracy:.1%}\")\n",
    "\n",
    "        print(f\"\\nConfusion matrix:\")\n",
    "        for c_true in [0, 1]:\n",
    "            for c_pred in [0, 1]:\n",
    "                count = np.sum((y_test == c_true) & (y_pred == c_pred))\n",
    "                print(f\"  True={class_names[c_true]:15s} Predicted={class_names[c_pred]:15s} : {count}\")\n",
    "\n",
    "        if accuracy > 0.85:\n",
    "            print(f\"\\nCorrect! The classifier achieves {accuracy:.1%} accuracy.\")\n",
    "    else:\n",
    "        print(\"Complete the predict_naive_bayes function!\")\n",
    "else:\n",
    "    print(\"Complete the previous functions first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b15781a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b> (click to reveal)</summary>\n",
    "\n",
    "```python\n",
    "def predict_naive_bayes(X, log_priors, log_likelihoods):\n",
    "    log_scores = X @ log_likelihoods.T + log_priors\n",
    "    predictions = np.argmax(log_scores, axis=1)\n",
    "    return predictions, log_scores\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fc8049",
   "metadata": {},
   "source": [
    "### Exercise 4: Interpret the classifier ★\n",
    "\n",
    "An advantage of naive Bayes: we can identify the most **discriminative** words for each class. The log-probability ratio $\\log \\theta_{d,c=1} - \\log \\theta_{d,c=0}$ indicates how strongly a word is associated with one class over the other.\n",
    "\n",
    "Complete the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339c09d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_discriminative_words(log_likelihoods, vocabulary, class_names, n_top=10):\n",
    "    \"\"\"\n",
    "    Find the most discriminative words for each class.\n",
    "\n",
    "    Args:\n",
    "        log_likelihoods: log conditional probabilities (C, D)\n",
    "        vocabulary: array of words (D,)\n",
    "        class_names: list of class names\n",
    "        n_top: number of words to display per class\n",
    "    \"\"\"\n",
    "    # ============================================\n",
    "    # TODO: Compute the ratio log(theta_d,c=1) - log(theta_d,c=0)\n",
    "    # Words with the largest ratios favor class 1\n",
    "    # Words with the smallest ratios favor class 0\n",
    "    # ============================================\n",
    "\n",
    "    log_ratio = None  # <- Replace with your code\n",
    "\n",
    "    if log_ratio is not None:\n",
    "        top_class1 = np.argsort(log_ratio)[-n_top:][::-1]\n",
    "        top_class0 = np.argsort(log_ratio)[:n_top]\n",
    "\n",
    "        print(f\"Words most associated with '{class_names[1]}':\")\n",
    "        for i in top_class1:\n",
    "            print(f\"  {vocabulary[i]:20s}  ratio = {log_ratio[i]:+.2f}\")\n",
    "\n",
    "        print(f\"\\nWords most associated with '{class_names[0]}':\")\n",
    "        for i in top_class0:\n",
    "            print(f\"  {vocabulary[i]:20s}  ratio = {log_ratio[i]:+.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea68664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the interpretation\n",
    "if log_lk is not None and not np.any(log_lk == 0):\n",
    "    most_discriminative_words(log_lk, vocabulary, class_names)\n",
    "else:\n",
    "    print(\"Complete the previous functions first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "face9040",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b> (click to reveal)</summary>\n",
    "\n",
    "```python\n",
    "def most_discriminative_words(log_likelihoods, vocabulary, class_names, n_top=10):\n",
    "    log_ratio = log_likelihoods[1] - log_likelihoods[0]\n",
    "\n",
    "    top_class1 = np.argsort(log_ratio)[-n_top:][::-1]\n",
    "    top_class0 = np.argsort(log_ratio)[:n_top]\n",
    "\n",
    "    print(f\"Words most associated with '{class_names[1]}':\")\n",
    "    for i in top_class1:\n",
    "        print(f\"  {vocabulary[i]:20s}  ratio = {log_ratio[i]:+.2f}\")\n",
    "\n",
    "    print(f\"\\nWords most associated with '{class_names[0]}':\")\n",
    "    for i in top_class0:\n",
    "        print(f\"  {vocabulary[i]:20s}  ratio = {log_ratio[i]:+.2f}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8011b40f",
   "metadata": {},
   "source": [
    "### Comparison with scikit-learn\n",
    "\n",
    "Let's verify that our implementation gives results similar to scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069aaf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "if 'y_pred' in dir() and y_pred is not None:\n",
    "    clf = MultinomialNB(alpha=1.0)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    accuracy_sklearn = clf.score(X_test, y_test)\n",
    "    y_pred_sklearn = clf.predict(X_test)\n",
    "\n",
    "    print(\"Accuracy comparison:\")\n",
    "    print(f\"  Our implementation: {np.mean(y_pred == y_test):.1%}\")\n",
    "    print(f\"  Scikit-learn:       {accuracy_sklearn:.1%}\")\n",
    "\n",
    "    agreement = np.mean(y_pred == y_pred_sklearn)\n",
    "    print(f\"\\nPrediction agreement: {agreement:.1%}\")\n",
    "else:\n",
    "    print(\"Complete the previous functions first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184dc253",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Latent Variables and Gaussian Mixture Models\n",
    "\n",
    "Let's move from supervised classification to **unsupervised clustering**. A Gaussian mixture model (GMM) assumes that the data is generated by $K$ Gaussian distributions:\n",
    "\n",
    "$$p(\\mathbf{x} \\mid \\boldsymbol{\\theta}) = \\sum_{k=1}^K \\pi_k \\, \\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$$\n",
    "\n",
    "A **latent variable** $z \\in \\{1, \\ldots, K\\}$ indicates which component each observation comes from. The EM algorithm estimates the parameters by alternating between computing the **responsibilities** (soft membership probabilities) and updating the parameters.\n",
    "\n",
    "We work with synthetic 2D data so we can visualize each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2825c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data from 3 Gaussian components\n",
    "np.random.seed(42)\n",
    "\n",
    "# True parameters (unknown to the algorithm)\n",
    "true_means = [np.array([0, 0]), np.array([5, 0]), np.array([2.5, 4])]\n",
    "true_covs = [\n",
    "    np.array([[1.0, 0.3], [0.3, 1.0]]),\n",
    "    np.array([[1.5, -0.5], [-0.5, 0.5]]),\n",
    "    np.array([[0.5, 0.0], [0.0, 2.0]])\n",
    "]\n",
    "true_weights = [0.3, 0.4, 0.3]\n",
    "n_total = 450\n",
    "\n",
    "# Generate the data\n",
    "X_gmm = []\n",
    "z_true = []\n",
    "for k in range(3):\n",
    "    n_k = int(true_weights[k] * n_total)\n",
    "    X_gmm.append(np.random.multivariate_normal(true_means[k], true_covs[k], n_k))\n",
    "    z_true.extend([k] * n_k)\n",
    "\n",
    "X_gmm = np.vstack(X_gmm)\n",
    "z_true = np.array(z_true)\n",
    "\n",
    "# Shuffle the data\n",
    "shuffle_idx = np.random.permutation(len(X_gmm))\n",
    "X_gmm = X_gmm[shuffle_idx]\n",
    "z_true = z_true[shuffle_idx]\n",
    "\n",
    "print(f\"Number of points: {len(X_gmm)}\")\n",
    "print(f\"Dimensions: {X_gmm.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac3329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.scatter(X_gmm[:, 0], X_gmm[:, 1], c='gray', alpha=0.4, s=15)\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_title('Observed data (no labels)')\n",
    "\n",
    "ax = axes[1]\n",
    "colors_true = ['steelblue', 'coral', 'seagreen']\n",
    "for k in range(3):\n",
    "    mask = z_true == k\n",
    "    ax.scatter(X_gmm[mask, 0], X_gmm[mask, 1], c=colors_true[k], alpha=0.5,\n",
    "               s=15, label=f'Component {k+1}')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_title(\"Ground truth (unknown to the algorithm)\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42627c45",
   "metadata": {},
   "source": [
    "### Exercise 5: Compute the multivariate Gaussian density ★\n",
    "\n",
    "The density of a multivariate Gaussian is:\n",
    "\n",
    "$$\\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{D/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) \\right)$$\n",
    "\n",
    "The term $(\\mathbf{x} - \\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})$ is the squared **Mahalanobis distance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2b2e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_pdf(X, mu, Sigma):\n",
    "    \"\"\"\n",
    "    Compute the multivariate Gaussian density for each point.\n",
    "\n",
    "    Args:\n",
    "        X: data (N, D)\n",
    "        mu: mean (D,)\n",
    "        Sigma: covariance matrix (D, D)\n",
    "\n",
    "    Returns:\n",
    "        densities: densities (N,)\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "\n",
    "    # ============================================\n",
    "    # TODO: Compute the Gaussian density\n",
    "    # 1. diff = X - mu                        (N, D)\n",
    "    # 2. Sigma_inv = np.linalg.inv(Sigma)      (D, D)\n",
    "    # 3. mahal = np.sum(diff @ Sigma_inv * diff, axis=1)  (N,)\n",
    "    # 4. norm = 1 / sqrt((2*pi)^D * det(Sigma))\n",
    "    # 5. densities = norm * exp(-0.5 * mahal)\n",
    "    # ============================================\n",
    "\n",
    "    densities = None  # <- Replace with your code\n",
    "\n",
    "    return densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bf229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: compare with scipy\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "test_pdf = gaussian_pdf(X_gmm[:2], np.array([0, 0]), np.eye(2))\n",
    "\n",
    "if test_pdf is not None:\n",
    "    mu_test = np.array([1.0, 2.0])\n",
    "    Sigma_test = np.array([[1.0, 0.3], [0.3, 0.5]])\n",
    "\n",
    "    our_result = gaussian_pdf(X_gmm[:5], mu_test, Sigma_test)\n",
    "    scipy_result = multivariate_normal.pdf(X_gmm[:5], mean=mu_test, cov=Sigma_test)\n",
    "\n",
    "    print(\"Comparison with scipy.stats:\")\n",
    "    for i in range(5):\n",
    "        print(f\"  Point {i}: ours = {our_result[i]:.6f}, scipy = {scipy_result[i]:.6f}\")\n",
    "\n",
    "    if np.allclose(our_result, scipy_result, atol=1e-10):\n",
    "        print(\"\\nCorrect!\")\n",
    "    else:\n",
    "        print(\"\\nCheck your implementation.\")\n",
    "else:\n",
    "    print(\"Complete the gaussian_pdf function!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4bc314",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b> (click to reveal)</summary>\n",
    "\n",
    "```python\n",
    "def gaussian_pdf(X, mu, Sigma):\n",
    "    N, D = X.shape\n",
    "    diff = X - mu\n",
    "    Sigma_inv = np.linalg.inv(Sigma)\n",
    "    mahal = np.sum(diff @ Sigma_inv * diff, axis=1)\n",
    "    norm = 1.0 / np.sqrt((2 * np.pi) ** D * np.linalg.det(Sigma))\n",
    "    densities = norm * np.exp(-0.5 * mahal)\n",
    "    return densities\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fbc846",
   "metadata": {},
   "source": [
    "### Exercise 6: Compute the responsibilities (E-step) ★★\n",
    "\n",
    "The **E-step** computes the responsibilities — the posterior probability that each point comes from each component:\n",
    "\n",
    "$$r_{nk} = \\frac{\\pi_k \\, \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{\\sum_{j=1}^K \\pi_j \\, \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)}$$\n",
    "\n",
    "The responsibilities are values in $[0, 1]$ that sum to 1 for each point. This is **soft clustering**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b31200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(X, weights, means, covariances):\n",
    "    \"\"\"\n",
    "    E-step: compute the responsibilities.\n",
    "\n",
    "    Args:\n",
    "        X: data (N, D)\n",
    "        weights: mixture weights (K,)\n",
    "        means: list of K means (each of dimension D)\n",
    "        covariances: list of K covariance matrices (each D×D)\n",
    "\n",
    "    Returns:\n",
    "        responsibilities: matrix (N, K) of responsibilities\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    K = len(weights)\n",
    "\n",
    "    # ============================================\n",
    "    # TODO: Compute the responsibilities\n",
    "    # 1. For each component k:\n",
    "    #    weighted_pdf[:, k] = weights[k] * gaussian_pdf(X, means[k], covariances[k])\n",
    "    # 2. Normalize each row:\n",
    "    #    responsibilities = weighted_pdf / weighted_pdf.sum(axis=1, keepdims=True)\n",
    "    # ============================================\n",
    "\n",
    "    responsibilities = None  # <- Replace with your code\n",
    "\n",
    "    return responsibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de4b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with the true parameters\n",
    "test_pdf = gaussian_pdf(X_gmm[:2], np.array([0, 0]), np.eye(2))\n",
    "\n",
    "if test_pdf is not None:\n",
    "    r_test = e_step(X_gmm, np.array(true_weights), true_means, true_covs)\n",
    "\n",
    "    if r_test is not None:\n",
    "        print(f\"Responsibilities shape: {r_test.shape}\")\n",
    "        print(f\"Row sums (first 5, expected: 1.0): {r_test[:5].sum(axis=1).round(4)}\")\n",
    "        print(f\"\\nResponsibilities (first 5 points):\")\n",
    "        print(r_test[:5].round(3))\n",
    "\n",
    "        if np.allclose(r_test.sum(axis=1), 1.0):\n",
    "            print(\"\\nCorrect! Responsibilities sum to 1 for each point.\")\n",
    "    else:\n",
    "        print(\"Complete the e_step function!\")\n",
    "else:\n",
    "    print(\"Complete gaussian_pdf first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507b1a78",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b> (click to reveal)</summary>\n",
    "\n",
    "```python\n",
    "def e_step(X, weights, means, covariances):\n",
    "    N = X.shape[0]\n",
    "    K = len(weights)\n",
    "\n",
    "    weighted_pdf = np.zeros((N, K))\n",
    "    for k in range(K):\n",
    "        weighted_pdf[:, k] = weights[k] * gaussian_pdf(X, means[k], covariances[k])\n",
    "\n",
    "    responsibilities = weighted_pdf / weighted_pdf.sum(axis=1, keepdims=True)\n",
    "    return responsibilities\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0df2b2b",
   "metadata": {},
   "source": [
    "### Exercise 7: Update the parameters (M-step) ★★\n",
    "\n",
    "The **M-step** updates the parameters using the responsibilities as weights. Let $N_k = \\sum_{n=1}^N r_{nk}$ be the effective number of points in component $k$:\n",
    "\n",
    "$$\\pi_k = \\frac{N_k}{N}, \\qquad \\boldsymbol{\\mu}_k = \\frac{1}{N_k} \\sum_{n=1}^N r_{nk} \\mathbf{x}_n$$\n",
    "\n",
    "$$\\boldsymbol{\\Sigma}_k = \\frac{1}{N_k} \\sum_{n=1}^N r_{nk} (\\mathbf{x}_n - \\boldsymbol{\\mu}_k)(\\mathbf{x}_n - \\boldsymbol{\\mu}_k)^\\top$$\n",
    "\n",
    "These formulas are **weighted** versions of the classical estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877df849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step(X, responsibilities):\n",
    "    \"\"\"\n",
    "    M-step: update the GMM parameters.\n",
    "\n",
    "    Args:\n",
    "        X: data (N, D)\n",
    "        responsibilities: matrix (N, K) of responsibilities\n",
    "\n",
    "    Returns:\n",
    "        weights: new weights (K,)\n",
    "        means: new means (list of K arrays)\n",
    "        covariances: new covariances (list of K matrices)\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = responsibilities.shape[1]\n",
    "\n",
    "    # ============================================\n",
    "    # TODO: Update the parameters\n",
    "    # 1. N_k = responsibilities.sum(axis=0)             (K,)\n",
    "    # 2. weights = N_k / N\n",
    "    # 3. For each k:\n",
    "    #    mu_k = (responsibilities[:, k:k+1] * X).sum(axis=0) / N_k[k]\n",
    "    #    diff = X - mu_k\n",
    "    #    Sigma_k = (responsibilities[:, k:k+1] * diff).T @ diff / N_k[k]\n",
    "    #    Sigma_k += 1e-6 * np.eye(D)  (regularization)\n",
    "    # ============================================\n",
    "\n",
    "    N_k = None  # <- Effective number of points per component\n",
    "    weights = None  # <- Mixture weights\n",
    "    means = []\n",
    "    covariances = []\n",
    "\n",
    "    for k in range(K):\n",
    "        mu_k = None  # <- Mean of component k\n",
    "        means.append(mu_k)\n",
    "\n",
    "        Sigma_k = None  # <- Covariance of component k\n",
    "        covariances.append(Sigma_k)\n",
    "\n",
    "    return weights, means, covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e44beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the M-step\n",
    "if 'r_test' in dir() and r_test is not None:\n",
    "    w_new, m_new, c_new = m_step(X_gmm, r_test)\n",
    "\n",
    "    if w_new is not None and m_new[0] is not None:\n",
    "        print(\"Updated parameters (with the true responsibilities):\")\n",
    "        for k in range(3):\n",
    "            print(f\"\\n  Component {k+1}:\")\n",
    "            print(f\"    Weight: {w_new[k]:.3f} (true: {true_weights[k]:.3f})\")\n",
    "            print(f\"    Mean: [{m_new[k][0]:.2f}, {m_new[k][1]:.2f}] \"\n",
    "                  f\"(true: [{true_means[k][0]:.1f}, {true_means[k][1]:.1f}])\")\n",
    "\n",
    "        if np.isclose(sum(w_new), 1.0):\n",
    "            print(\"\\nCorrect! The weights sum to 1.\")\n",
    "    else:\n",
    "        print(\"Complete the m_step function!\")\n",
    "else:\n",
    "    print(\"Complete e_step first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d494eb7",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b> (click to reveal)</summary>\n",
    "\n",
    "```python\n",
    "def m_step(X, responsibilities):\n",
    "    N, D = X.shape\n",
    "    K = responsibilities.shape[1]\n",
    "\n",
    "    N_k = responsibilities.sum(axis=0)\n",
    "    weights = N_k / N\n",
    "    means = []\n",
    "    covariances = []\n",
    "\n",
    "    for k in range(K):\n",
    "        mu_k = (responsibilities[:, k:k+1] * X).sum(axis=0) / N_k[k]\n",
    "        means.append(mu_k)\n",
    "\n",
    "        diff = X - mu_k\n",
    "        Sigma_k = (responsibilities[:, k:k+1] * diff).T @ diff / N_k[k]\n",
    "        Sigma_k += 1e-6 * np.eye(D)\n",
    "        covariances.append(Sigma_k)\n",
    "\n",
    "    return weights, means, covariances\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b60d9",
   "metadata": {},
   "source": [
    "### Exercise 8: Implement the complete EM algorithm ★★\n",
    "\n",
    "The EM algorithm alternates E and M steps until convergence. We measure convergence by the **log-likelihood**:\n",
    "\n",
    "$$\\log p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) = \\sum_{n=1}^N \\log \\left( \\sum_{k=1}^K \\pi_k \\, \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\right)$$\n",
    "\n",
    "The `compute_log_likelihood` function is provided. Complete the EM loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be30ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood(X, weights, means, covariances):\n",
    "    \"\"\"Compute the log-likelihood of the model.\"\"\"\n",
    "    N = X.shape[0]\n",
    "    K = len(weights)\n",
    "\n",
    "    weighted_pdfs = np.zeros((N, K))\n",
    "    for k in range(K):\n",
    "        weighted_pdfs[:, k] = weights[k] * gaussian_pdf(X, means[k], covariances[k])\n",
    "\n",
    "    return np.sum(np.log(weighted_pdfs.sum(axis=1)))\n",
    "\n",
    "\n",
    "def em_algorithm(X, K, n_iterations=50, seed=0):\n",
    "    \"\"\"\n",
    "    EM algorithm for a Gaussian mixture model.\n",
    "\n",
    "    Args:\n",
    "        X: data (N, D)\n",
    "        K: number of components\n",
    "        n_iterations: maximum number of iterations\n",
    "        seed: random seed for initialization\n",
    "\n",
    "    Returns:\n",
    "        weights, means, covariances: learned parameters\n",
    "        responsibilities: final responsibilities (N, K)\n",
    "        log_likelihoods: log-likelihood history\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Initialization: K random points as means\n",
    "    indices = np.random.choice(N, K, replace=False)\n",
    "    means = [X[idx].copy() for idx in indices]\n",
    "    covariances = [np.eye(D) for _ in range(K)]\n",
    "    weights = np.ones(K) / K\n",
    "\n",
    "    log_likelihoods = []\n",
    "\n",
    "    # ============================================\n",
    "    # TODO: Implement the EM loop\n",
    "    # For each iteration:\n",
    "    # 1. responsibilities = e_step(X, weights, means, covariances)\n",
    "    # 2. weights, means, covariances = m_step(X, responsibilities)\n",
    "    # 3. ll = compute_log_likelihood(X, weights, means, covariances)\n",
    "    # 4. log_likelihoods.append(ll)\n",
    "    # ============================================\n",
    "\n",
    "    for t in range(n_iterations):\n",
    "        # E-step\n",
    "        responsibilities = None  # <- Complete\n",
    "\n",
    "        # M-step\n",
    "        # <- Complete (update weights, means, covariances)\n",
    "        pass\n",
    "\n",
    "        # Log-likelihood\n",
    "        ll = None  # <- Complete\n",
    "        log_likelihoods.append(ll)\n",
    "\n",
    "    return weights, means, covariances, responsibilities, log_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e9371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the EM algorithm\n",
    "test_pdf = gaussian_pdf(X_gmm[:2], np.array([0, 0]), np.eye(2))\n",
    "\n",
    "if test_pdf is not None:\n",
    "    weights_em, means_em, covs_em, resp_em, ll_history = em_algorithm(\n",
    "        X_gmm, K=3, n_iterations=50, seed=42\n",
    "    )\n",
    "\n",
    "    if ll_history[0] is not None:\n",
    "        print(\"Parameters learned by EM:\")\n",
    "        for k in range(3):\n",
    "            print(f\"\\n  Component {k+1}:\")\n",
    "            print(f\"    Weight: {weights_em[k]:.3f}\")\n",
    "            print(f\"    Mean: [{means_em[k][0]:.2f}, {means_em[k][1]:.2f}]\")\n",
    "\n",
    "        print(f\"\\nInitial log-likelihood: {ll_history[0]:.1f}\")\n",
    "        print(f\"Final log-likelihood:   {ll_history[-1]:.1f}\")\n",
    "    else:\n",
    "        print(\"Complete the em_algorithm function!\")\n",
    "else:\n",
    "    print(\"Complete gaussian_pdf first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c271304",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b> (click to reveal)</summary>\n",
    "\n",
    "```python\n",
    "def em_algorithm(X, K, n_iterations=50, seed=0):\n",
    "    N, D = X.shape\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    indices = np.random.choice(N, K, replace=False)\n",
    "    means = [X[idx].copy() for idx in indices]\n",
    "    covariances = [np.eye(D) for _ in range(K)]\n",
    "    weights = np.ones(K) / K\n",
    "\n",
    "    log_likelihoods = []\n",
    "\n",
    "    for t in range(n_iterations):\n",
    "        responsibilities = e_step(X, weights, means, covariances)\n",
    "        weights, means, covariances = m_step(X, responsibilities)\n",
    "        ll = compute_log_likelihood(X, weights, means, covariances)\n",
    "        log_likelihoods.append(ll)\n",
    "\n",
    "    return weights, means, covariances, responsibilities, log_likelihoods\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabf5d53",
   "metadata": {},
   "source": [
    "### Visualize convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431467cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'll_history' in dir() and len(ll_history) > 0 and ll_history[0] is not None:\n",
    "    from matplotlib.patches import Ellipse\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Log-likelihood\n",
    "    ax = axes[0]\n",
    "    ax.plot(ll_history, 'C0-', linewidth=2)\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Log-likelihood')\n",
    "    ax.set_title(\"EM Algorithm Convergence\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Clustering result\n",
    "    ax = axes[1]\n",
    "    z_em = np.argmax(resp_em, axis=1)\n",
    "    colors_em = ['steelblue', 'coral', 'seagreen']\n",
    "\n",
    "    for k in range(3):\n",
    "        mask = z_em == k\n",
    "        ax.scatter(X_gmm[mask, 0], X_gmm[mask, 1], c=colors_em[k], alpha=0.4,\n",
    "                   s=15, label=f'Component {k+1} ($\\pi$={weights_em[k]:.2f})')\n",
    "\n",
    "        # Draw ellipses\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(covs_em[k])\n",
    "        angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))\n",
    "        for n_std in [1, 2]:\n",
    "            width = 2 * n_std * np.sqrt(eigenvalues[0])\n",
    "            height = 2 * n_std * np.sqrt(eigenvalues[1])\n",
    "            ellipse = Ellipse(means_em[k], width, height, angle=angle, fill=False,\n",
    "                             color=colors_em[k], linewidth=2, linestyle='--')\n",
    "            ax.add_patch(ellipse)\n",
    "\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    ax.set_title(\"EM Algorithm Result\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Complete the EM algorithm first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af624773",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: GMM vs. K-Means Comparison\n",
    "\n",
    "The k-means algorithm is a limiting case of GMM where the covariances are spherical and identical, and the responsibilities become binary. Let's compare the two approaches on data with clusters of **different elliptical shapes**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c280a56",
   "metadata": {},
   "source": [
    "### Exercise 9: GMM vs. K-Means ★★★\n",
    "\n",
    "Run the cell below, then answer the reflection questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58d7218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "if 'll_history' in dir() and len(ll_history) > 0 and ll_history[0] is not None:\n",
    "    # Data with elongated ellipses\n",
    "    np.random.seed(123)\n",
    "    X_ellipse = np.vstack([\n",
    "        np.random.multivariate_normal([0, 0], [[4, 0], [0, 0.2]], 150),\n",
    "        np.random.multivariate_normal([0, 3], [[0.2, 0], [0, 4]], 150),\n",
    "    ])\n",
    "\n",
    "    # K-means\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "    z_kmeans = kmeans.fit_predict(X_ellipse)\n",
    "\n",
    "    # GMM (our implementation)\n",
    "    w_cmp, m_cmp, c_cmp, r_cmp, _ = em_algorithm(X_ellipse, K=2, n_iterations=50, seed=42)\n",
    "    z_gmm = np.argmax(r_cmp, axis=1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # K-means\n",
    "    ax = axes[0]\n",
    "    for k in range(2):\n",
    "        mask = z_kmeans == k\n",
    "        ax.scatter(X_ellipse[mask, 0], X_ellipse[mask, 1], c=f'C{k}', alpha=0.5, s=15)\n",
    "    ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "               c='black', marker='X', s=200, zorder=5, label='Centroids')\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    ax.set_title('K-Means')\n",
    "    ax.legend()\n",
    "\n",
    "    # GMM\n",
    "    ax = axes[1]\n",
    "    for k in range(2):\n",
    "        mask = z_gmm == k\n",
    "        ax.scatter(X_ellipse[mask, 0], X_ellipse[mask, 1], c=f'C{k}', alpha=0.5, s=15)\n",
    "\n",
    "    for k in range(2):\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(c_cmp[k])\n",
    "        angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))\n",
    "        for n_std in [1, 2]:\n",
    "            width = 2 * n_std * np.sqrt(np.maximum(eigenvalues[0], 1e-6))\n",
    "            height = 2 * n_std * np.sqrt(np.maximum(eigenvalues[1], 1e-6))\n",
    "            ellipse = Ellipse(m_cmp[k], width, height, angle=angle, fill=False,\n",
    "                             color=f'C{k}', linewidth=2, linestyle='--')\n",
    "            ax.add_patch(ellipse)\n",
    "\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    ax.set_title('GMM (EM Algorithm)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Complete the EM algorithm first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fc7896",
   "metadata": {},
   "source": [
    "**Reflection questions:**\n",
    "1. K-means partitions the space into regions bounded by linear boundaries (Voronoi diagram). Why is this problematic for elongated clusters?\n",
    "2. GMM models each component with its own covariance matrix. What advantage does this give on data with varied shapes?\n",
    "3. The soft responsibilities of GMM indicate assignment uncertainty. In which practical applications would this information be useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb0274f",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this practical exercise, you implemented two generative models:\n",
    "\n",
    "1. **Naive Bayes** for text classification:\n",
    "   - Estimation of priors $\\hat{\\pi}_c = N_c / N$ and conditional probabilities $\\hat{\\theta}_{dc}$ by maximum likelihood\n",
    "   - Laplace smoothing to avoid zero probabilities\n",
    "   - Prediction in log-space: $\\hat{y} = \\arg\\max_c [\\log \\pi_c + \\sum_d x_d \\log \\theta_{dc}]$\n",
    "\n",
    "2. **Gaussian Mixture Model** for unsupervised clustering:\n",
    "   - Multivariate Gaussian density and Mahalanobis distance\n",
    "   - Responsibilities: soft membership probabilities $r_{nk} = p(z_n = k \\mid \\mathbf{x}_n)$\n",
    "   - EM algorithm: alternating E-step (responsibilities) and M-step (weighted parameters)\n",
    "\n",
    "Naive Bayes illustrates the generative approach in supervised classification: modeling $p(\\mathbf{x} \\mid y)$ for each class rather than $p(y \\mid \\mathbf{x})$ directly. GMM extends this idea to the unsupervised case, where the \"classes\" (components) are unknown and must be inferred by the EM algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "**Going further**: [Chapter 6: Generative Probabilistic Models](https://pierrelux.github.io/mlbook/ch6_probabilistic_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ift-6390-machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
