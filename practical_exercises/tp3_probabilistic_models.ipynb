{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# TP3: Modèles probabilistes génératifs\n\n**IFT6390 - Fondements de l'apprentissage machine**\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pierrelux/mlbook/blob/main/exercises/tp3_probabilistic_models.ipynb)\n\nCe notebook accompagne le [Chapitre 6: Modèles probabilistes génératifs](https://pierrelux.github.io/mlbook/ch6_probabilistic_models).\n\n## Objectifs\n\nÀ la fin de ce TP, vous serez en mesure de:\n- Estimer les paramètres d'un classifieur naïf bayésien par maximum de vraisemblance\n- Appliquer le lissage de Laplace pour éviter les probabilités nulles\n- Implémenter la prédiction en espace logarithmique pour la stabilité numérique\n- Calculer la densité gaussienne multivariée\n- Implémenter l'algorithme EM pour un modèle de mélange gaussien\n- Comparer le partitionnement souple (GMM) et dur (k-moyennes)\n\nCe TP implémente **tout à la main**, sans utiliser scikit-learn pour l'entraînement. Scikit-learn n'est utilisé que pour charger les données et pour une comparaison finale.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Partie 0: Configuration\n\nExécutez cette cellule pour importer les bibliothèques nécessaires."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Pour de jolis graphiques\nplt.rcParams['figure.figsize'] = (8, 5)\nplt.rcParams['font.size'] = 12\n\nprint(\"Configuration terminée!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Partie 1: Données de classification de texte\n\nLe classifieur naïf bayésien est particulièrement populaire pour la **classification de texte**. Nous utilisons un sous-ensemble du jeu de données **20 Newsgroups**, qui contient des messages de forums de discussion regroupés par sujet.\n\nNotre tâche: distinguer les messages portant sur la **médecine** de ceux portant sur l'**électronique**, à partir des mots qu'ils contiennent."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Charger deux catégories\ncategories = ['sci.electronics', 'sci.med']\nnewsgroups = fetch_20newsgroups(subset='all', categories=categories,\n                                shuffle=True, random_state=42)\n\n# Noms des classes (en français)\nnoms_classes = ['Électronique', 'Médecine']\n\n# Convertir les textes en matrice de comptage de mots\nvectorizer = CountVectorizer(max_features=2000, stop_words='english')\nX_counts = vectorizer.fit_transform(newsgroups.data).toarray()\ny = newsgroups.target\nvocabulaire = vectorizer.get_feature_names_out()\n\nprint(f\"Nombre de documents: {len(y)}\")\nprint(f\"Classes: {newsgroups.target_names}\")\nprint(f\"Matrice de comptage: {X_counts.shape} ({X_counts.shape[0]} documents × {X_counts.shape[1]} mots)\")\nprint(f\"\\nExemple — premiers 10 mots du vocabulaire: {list(vocabulaire[:10])}\")\nprint(f\"Comptages correspondants (document 0): {X_counts[0, :10]}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Séparation entraînement / test (80% / 20%)\nnp.random.seed(42)\nn = len(y)\nindices = np.random.permutation(n)\nn_train = int(0.8 * n)\n\ntrain_idx = indices[:n_train]\ntest_idx = indices[n_train:]\n\nX_train, y_train = X_counts[train_idx], y[train_idx]\nX_test, y_test = X_counts[test_idx], y[test_idx]\n\nprint(f\"Entraînement: {len(X_train)} documents\")\nprint(f\"Test: {len(X_test)} documents\")\nprint(f\"\\nRépartition (entraînement):\")\nfor c in [0, 1]:\n    print(f\"  {noms_classes[c]}: {np.sum(y_train == c)} documents ({np.mean(y_train == c):.1%})\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Mots les plus fréquents par classe\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nfor c, ax in enumerate(axes):\n    mask = y_train == c\n    word_counts = X_train[mask].sum(axis=0)\n    top_indices = np.argsort(word_counts)[-15:]\n\n    ax.barh(range(15), word_counts[top_indices], color=f'C{c}', alpha=0.7)\n    ax.set_yticks(range(15))\n    ax.set_yticklabels([vocabulaire[i] for i in top_indices])\n    ax.set_xlabel(\"Nombre total d'occurrences\")\n    ax.set_title(f'{noms_classes[c]}')\n\nplt.suptitle('Mots les plus fréquents par classe', fontsize=14)\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Partie 2: Le classifieur naïf bayésien\n\nLe classifieur naïf bayésien suppose que les mots apparaissent **indépendamment** les uns des autres, conditionnellement à la classe. Pour la classification de texte avec des comptages de mots, nous utilisons le modèle **multinomial**:\n\n$$\\hat{y} = \\arg\\max_c \\left[ \\log \\pi_c + \\sum_{d=1}^{D} x_d \\log \\theta_{dc} \\right]$$\n\noù:\n- $\\pi_c = p(y = c)$ est la probabilité a priori de la classe $c$\n- $\\theta_{dc} = p(\\text{mot } d \\mid y = c)$ est la probabilité du mot $d$ dans la classe $c$\n- $x_d$ est le nombre d'occurrences du mot $d$ dans le document"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercice 1: Estimer les a priori de classe ★\n\nLa première étape est d'estimer les probabilités a priori $\\pi_c$. L'estimateur du maximum de vraisemblance est la fréquence de chaque classe:\n\n$$\\hat{\\pi}_c = \\frac{N_c}{N}$$\n\nNous travaillons en espace logarithmique pour la stabilité numérique. Complétez la fonction ci-dessous."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def estimate_log_priors(y_train):\n    \"\"\"\n    Estime les log-probabilités a priori de chaque classe.\n\n    Args:\n        y_train: étiquettes d'entraînement (N,)\n\n    Returns:\n        log_priors: array de taille (C,) contenant log(pi_c) pour chaque classe\n    \"\"\"\n    classes = np.unique(y_train)\n    N = len(y_train)\n\n    # ============================================\n    # TODO: Calculez les log-probabilités a priori\n    # Indice: log(N_c / N) pour chaque classe c\n    # ============================================\n\n    log_priors = None  # <- Remplacez par votre code\n\n    return log_priors",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Test de votre fonction\nlog_priors = estimate_log_priors(y_train)\n\nif log_priors is not None:\n    for c in range(len(log_priors)):\n        print(f\"log pi({noms_classes[c]}) = {log_priors[c]:.4f}  ->  pi = {np.exp(log_priors[c]):.4f}\")\n\n    # Vérification: les probabilités doivent sommer à 1\n    if np.isclose(np.sum(np.exp(log_priors)), 1.0):\n        print(\"\\nCorrect! Les probabilités somment à 1.\")\n    else:\n        print(\"\\nVérifiez votre implémentation.\")\nelse:\n    print(\"Complétez la fonction estimate_log_priors!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary><b>Solution</b> (cliquez pour afficher)</summary>\n\n```python\ndef estimate_log_priors(y_train):\n    classes = np.unique(y_train)\n    N = len(y_train)\n    log_priors = np.array([np.log(np.sum(y_train == c) / N) for c in classes])\n    return log_priors\n```\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercice 2: Estimer les probabilités conditionnelles ★\n\nPour le modèle multinomial, la probabilité du mot $d$ dans la classe $c$ est estimée par:\n\n$$\\hat{\\theta}_{dc} = \\frac{N_{dc} + \\alpha}{N_c^{\\text{mots}} + \\alpha V}$$\n\noù:\n- $N_{dc}$ = nombre total d'occurrences du mot $d$ dans les documents de classe $c$\n- $N_c^{\\text{mots}} = \\sum_d N_{dc}$ = nombre total de mots dans la classe $c$\n- $V$ = taille du vocabulaire\n- $\\alpha$ = paramètre de lissage de Laplace ($\\alpha = 1$ par défaut)\n\nLe lissage de Laplace évite les probabilités nulles: sans lui, un seul mot absent suffirait à éliminer une classe."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def estimate_log_likelihoods(X_train, y_train, alpha=1.0):\n    \"\"\"\n    Estime les log-probabilités conditionnelles de chaque mot par classe.\n\n    Args:\n        X_train: matrice de comptage (N, D)\n        y_train: étiquettes (N,)\n        alpha: paramètre de lissage de Laplace\n\n    Returns:\n        log_likelihoods: matrice (C, D) où l'entrée [c, d] = log(theta_dc)\n    \"\"\"\n    classes = np.unique(y_train)\n    C = len(classes)\n    D = X_train.shape[1]\n    V = D  # taille du vocabulaire\n\n    log_likelihoods = np.zeros((C, D))\n\n    for c in classes:\n        # Documents de la classe c\n        X_c = X_train[y_train == c]\n\n        # ============================================\n        # TODO: Calculez les log-probabilités conditionnelles\n        # 1. N_dc = X_c.sum(axis=0)  (occurrences de chaque mot)\n        # 2. N_c_mots = N_dc.sum()  (total de mots dans la classe)\n        # 3. theta_dc = (N_dc + alpha) / (N_c_mots + alpha * V)\n        # 4. log_likelihoods[c] = np.log(theta_dc)\n        # ============================================\n\n        log_likelihoods[c] = None  # <- Remplacez par votre code\n\n    return log_likelihoods",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Test de votre fonction\nlog_lk = estimate_log_likelihoods(X_train, y_train, alpha=1.0)\n\nif log_lk is not None and not np.any(log_lk == 0):\n    print(f\"Forme de la matrice: {log_lk.shape} (classes × mots)\")\n\n    for c in range(2):\n        prob_sum = np.exp(log_lk[c]).sum()\n        print(f\"\\nClasse {noms_classes[c]}:\")\n        print(f\"  Somme des probabilités: {prob_sum:.4f} (attendu: ≈ 1.0)\")\n\n        top5 = np.argsort(log_lk[c])[-5:][::-1]\n        print(f\"  Top 5 mots: {[vocabulaire[i] for i in top5]}\")\n\n    if all(np.isclose(np.exp(log_lk[c]).sum(), 1.0, atol=1e-5) for c in range(2)):\n        print(\"\\nCorrect!\")\nelse:\n    print(\"Complétez la fonction estimate_log_likelihoods!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary><b>Solution</b> (cliquez pour afficher)</summary>\n\n```python\ndef estimate_log_likelihoods(X_train, y_train, alpha=1.0):\n    classes = np.unique(y_train)\n    C = len(classes)\n    D = X_train.shape[1]\n    V = D\n\n    log_likelihoods = np.zeros((C, D))\n\n    for c in classes:\n        X_c = X_train[y_train == c]\n        N_dc = X_c.sum(axis=0)\n        N_c_mots = N_dc.sum()\n        theta_dc = (N_dc + alpha) / (N_c_mots + alpha * V)\n        log_likelihoods[c] = np.log(theta_dc)\n\n    return log_likelihoods\n```\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercice 3: Prédire avec le naïf bayésien ★★\n\nPour classifier un document, nous calculons le score de chaque classe et choisissons la plus probable:\n\n$$\\hat{y} = \\arg\\max_c \\left[ \\log \\pi_c + \\sum_{d=1}^{D} x_d \\log \\theta_{dc} \\right]$$\n\nEn forme matricielle: $\\text{scores} = \\mathbf{X} \\cdot \\log\\boldsymbol{\\Theta}^\\top + \\log\\boldsymbol{\\pi}$\n\nLe calcul en espace logarithmique évite les problèmes de sous-dépassement numérique: le produit de milliers de probabilités proches de zéro donnerait un résultat indistinguable de zéro en arithmétique flottante."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def predict_naive_bayes(X, log_priors, log_likelihoods):\n    \"\"\"\n    Prédit les classes avec le classifieur naïf bayésien.\n\n    Args:\n        X: matrice de comptage (N, D)\n        log_priors: log-probabilités a priori (C,)\n        log_likelihoods: log-probabilités conditionnelles (C, D)\n\n    Returns:\n        predictions: classes prédites (N,)\n        log_scores: matrice de scores (N, C)\n    \"\"\"\n    # ============================================\n    # TODO: Calculez les scores log pour chaque classe\n    # scores = X @ log_likelihoods.T + log_priors\n    # predictions = argmax des scores par ligne\n    # ============================================\n\n    log_scores = None  # <- Remplacez par votre code\n    predictions = None  # <- Remplacez par votre code\n\n    return predictions, log_scores",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Test de votre fonction\nif log_priors is not None and log_lk is not None and not np.any(log_lk == 0):\n    y_pred, scores = predict_naive_bayes(X_test, log_priors, log_lk)\n\n    if y_pred is not None:\n        accuracy = np.mean(y_pred == y_test)\n        print(f\"Précision sur le test: {accuracy:.1%}\")\n\n        print(f\"\\nMatrice de confusion:\")\n        for c_true in [0, 1]:\n            for c_pred in [0, 1]:\n                count = np.sum((y_test == c_true) & (y_pred == c_pred))\n                print(f\"  Vrai={noms_classes[c_true]:15s} Prédit={noms_classes[c_pred]:15s} : {count}\")\n\n        if accuracy > 0.85:\n            print(f\"\\nCorrect! Le classifieur atteint {accuracy:.1%} de précision.\")\n    else:\n        print(\"Complétez la fonction predict_naive_bayes!\")\nelse:\n    print(\"Complétez d'abord les fonctions précédentes!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary><b>Solution</b> (cliquez pour afficher)</summary>\n\n```python\ndef predict_naive_bayes(X, log_priors, log_likelihoods):\n    log_scores = X @ log_likelihoods.T + log_priors\n    predictions = np.argmax(log_scores, axis=1)\n    return predictions, log_scores\n```\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercice 4: Interpréter le classifieur ★\n\nUn avantage du naïf bayésien: nous pouvons identifier les mots les plus **discriminants** pour chaque classe. Le rapport des log-probabilités $\\log \\theta_{d,c=1} - \\log \\theta_{d,c=0}$ indique à quel point un mot est associé à une classe plutôt qu'à l'autre.\n\nComplétez la fonction ci-dessous."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def most_discriminative_words(log_likelihoods, vocabulaire, noms_classes, n_top=10):\n    \"\"\"\n    Trouve les mots les plus discriminants pour chaque classe.\n\n    Args:\n        log_likelihoods: log-probabilités conditionnelles (C, D)\n        vocabulaire: array de mots (D,)\n        noms_classes: liste de noms de classes\n        n_top: nombre de mots à afficher par classe\n    \"\"\"\n    # ============================================\n    # TODO: Calculez le ratio log(theta_d,c=1) - log(theta_d,c=0)\n    # Les mots avec les plus grands ratios favorisent la classe 1\n    # Les mots avec les plus petits ratios favorisent la classe 0\n    # ============================================\n\n    log_ratio = None  # <- Remplacez par votre code\n\n    if log_ratio is not None:\n        top_class1 = np.argsort(log_ratio)[-n_top:][::-1]\n        top_class0 = np.argsort(log_ratio)[:n_top]\n\n        print(f\"Mots les plus associés à '{noms_classes[1]}':\")\n        for i in top_class1:\n            print(f\"  {vocabulaire[i]:20s}  ratio = {log_ratio[i]:+.2f}\")\n\n        print(f\"\\nMots les plus associés à '{noms_classes[0]}':\")\n        for i in top_class0:\n            print(f\"  {vocabulaire[i]:20s}  ratio = {log_ratio[i]:+.2f}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Exécuter l'interprétation\nif log_lk is not None and not np.any(log_lk == 0):\n    most_discriminative_words(log_lk, vocabulaire, noms_classes)\nelse:\n    print(\"Complétez d'abord les fonctions précédentes!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary><b>Solution</b> (cliquez pour afficher)</summary>\n\n```python\ndef most_discriminative_words(log_likelihoods, vocabulaire, noms_classes, n_top=10):\n    log_ratio = log_likelihoods[1] - log_likelihoods[0]\n\n    top_class1 = np.argsort(log_ratio)[-n_top:][::-1]\n    top_class0 = np.argsort(log_ratio)[:n_top]\n\n    print(f\"Mots les plus associés à '{noms_classes[1]}':\")\n    for i in top_class1:\n        print(f\"  {vocabulaire[i]:20s}  ratio = {log_ratio[i]:+.2f}\")\n\n    print(f\"\\nMots les plus associés à '{noms_classes[0]}':\")\n    for i in top_class0:\n        print(f\"  {vocabulaire[i]:20s}  ratio = {log_ratio[i]:+.2f}\")\n```\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Comparaison avec scikit-learn\n\nVérifions que notre implémentation donne des résultats similaires à scikit-learn."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from sklearn.naive_bayes import MultinomialNB\n\nif 'y_pred' in dir() and y_pred is not None:\n    clf = MultinomialNB(alpha=1.0)\n    clf.fit(X_train, y_train)\n\n    accuracy_sklearn = clf.score(X_test, y_test)\n    y_pred_sklearn = clf.predict(X_test)\n\n    print(\"Comparaison des précisions:\")\n    print(f\"  Notre implémentation: {np.mean(y_pred == y_test):.1%}\")\n    print(f\"  Scikit-learn:         {accuracy_sklearn:.1%}\")\n\n    accord = np.mean(y_pred == y_pred_sklearn)\n    print(f\"\\nAccord des prédictions: {accord:.1%}\")\nelse:\n    print(\"Complétez d'abord les fonctions précédentes!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Partie 3: Variables latentes et modèles de mélange gaussien\n\nPassons de la classification supervisée au **partitionnement non supervisé**. Un modèle de mélange gaussien (GMM) suppose que les données sont générées par $K$ distributions gaussiennes:\n\n$$p(\\mathbf{x} \\mid \\boldsymbol{\\theta}) = \\sum_{k=1}^K \\pi_k \\, \\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$$\n\nUne **variable latente** $z \\in \\{1, \\ldots, K\\}$ indique de quel composant provient chaque observation. L'algorithme EM estime les paramètres en alternant entre le calcul des **responsabilités** (probabilités d'appartenance souples) et la mise à jour des paramètres.\n\nNous travaillons avec des données 2D synthétiques pour pouvoir visualiser chaque étape."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Générer des données de 3 composants gaussiens\nnp.random.seed(42)\n\n# Vrais paramètres (inconnus de l'algorithme)\ntrue_means = [np.array([0, 0]), np.array([5, 0]), np.array([2.5, 4])]\ntrue_covs = [\n    np.array([[1.0, 0.3], [0.3, 1.0]]),\n    np.array([[1.5, -0.5], [-0.5, 0.5]]),\n    np.array([[0.5, 0.0], [0.0, 2.0]])\n]\ntrue_weights = [0.3, 0.4, 0.3]\nn_total = 450\n\n# Générer les données\nX_gmm = []\nz_true = []\nfor k in range(3):\n    n_k = int(true_weights[k] * n_total)\n    X_gmm.append(np.random.multivariate_normal(true_means[k], true_covs[k], n_k))\n    z_true.extend([k] * n_k)\n\nX_gmm = np.vstack(X_gmm)\nz_true = np.array(z_true)\n\n# Mélanger les données\nshuffle_idx = np.random.permutation(len(X_gmm))\nX_gmm = X_gmm[shuffle_idx]\nz_true = z_true[shuffle_idx]\n\nprint(f\"Nombre de points: {len(X_gmm)}\")\nprint(f\"Dimensions: {X_gmm.shape[1]}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Visualisation\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nax = axes[0]\nax.scatter(X_gmm[:, 0], X_gmm[:, 1], c='gray', alpha=0.4, s=15)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_title('Données observées (sans étiquettes)')\n\nax = axes[1]\ncolors_true = ['steelblue', 'coral', 'seagreen']\nfor k in range(3):\n    mask = z_true == k\n    ax.scatter(X_gmm[mask, 0], X_gmm[mask, 1], c=colors_true[k], alpha=0.5,\n               s=15, label=f'Composant {k+1}')\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_title(\"Vérité (inconnue de l'algorithme)\")\nax.legend()\n\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercice 5: Calculer la densité gaussienne multivariée ★\n\nLa densité d'une gaussienne multivariée est:\n\n$$\\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{D/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) \\right)$$\n\nLe terme $(\\mathbf{x} - \\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})$ est la **distance de Mahalanobis** au carré."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def gaussian_pdf(X, mu, Sigma):\n    \"\"\"\n    Calcule la densité gaussienne multivariée pour chaque point.\n\n    Args:\n        X: données (N, D)\n        mu: moyenne (D,)\n        Sigma: matrice de covariance (D, D)\n\n    Returns:\n        densities: densités (N,)\n    \"\"\"\n    N, D = X.shape\n\n    # ============================================\n    # TODO: Calculez la densité gaussienne\n    # 1. diff = X - mu                        (N, D)\n    # 2. Sigma_inv = np.linalg.inv(Sigma)      (D, D)\n    # 3. mahal = np.sum(diff @ Sigma_inv * diff, axis=1)  (N,)\n    # 4. norm = 1 / sqrt((2*pi)^D * det(Sigma))\n    # 5. densities = norm * exp(-0.5 * mahal)\n    # ============================================\n\n    densities = None  # <- Remplacez par votre code\n\n    return densities",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Test: comparer avec scipy\nfrom scipy.stats import multivariate_normal\n\ntest_pdf = gaussian_pdf(X_gmm[:2], np.array([0, 0]), np.eye(2))\n\nif test_pdf is not None:\n    mu_test = np.array([1.0, 2.0])\n    Sigma_test = np.array([[1.0, 0.3], [0.3, 0.5]])\n\n    our_result = gaussian_pdf(X_gmm[:5], mu_test, Sigma_test)\n    scipy_result = multivariate_normal.pdf(X_gmm[:5], mean=mu_test, cov=Sigma_test)\n\n    print(\"Comparaison avec scipy.stats:\")\n    for i in range(5):\n        print(f\"  Point {i}: nôtre = {our_result[i]:.6f}, scipy = {scipy_result[i]:.6f}\")\n\n    if np.allclose(our_result, scipy_result, atol=1e-10):\n        print(\"\\nCorrect!\")\n    else:\n        print(\"\\nVérifiez votre implémentation.\")\nelse:\n    print(\"Complétez la fonction gaussian_pdf!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary><b>Solution</b> (cliquez pour afficher)</summary>\n\n```python\ndef gaussian_pdf(X, mu, Sigma):\n    N, D = X.shape\n    diff = X - mu\n    Sigma_inv = np.linalg.inv(Sigma)\n    mahal = np.sum(diff @ Sigma_inv * diff, axis=1)\n    norm = 1.0 / np.sqrt((2 * np.pi) ** D * np.linalg.det(Sigma))\n    densities = norm * np.exp(-0.5 * mahal)\n    return densities\n```\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercice 6: Calculer les responsabilités (étape E) ★★\n\nL'**étape E** calcule les responsabilités — la probabilité a posteriori que chaque point provienne de chaque composant:\n\n$$r_{nk} = \\frac{\\pi_k \\, \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{\\sum_{j=1}^K \\pi_j \\, \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)}$$\n\nLes responsabilités sont des valeurs dans $[0, 1]$ qui somment à 1 pour chaque point. C'est un **partitionnement souple**."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def e_step(X, weights, means, covariances):\n    \"\"\"\n    Étape E: calcule les responsabilités.\n\n    Args:\n        X: données (N, D)\n        weights: poids du mélange (K,)\n        means: liste de K moyennes (chacune de dimension D)\n        covariances: liste de K matrices de covariance (chacune D×D)\n\n    Returns:\n        responsibilities: matrice (N, K) des responsabilités\n    \"\"\"\n    N = X.shape[0]\n    K = len(weights)\n\n    # ============================================\n    # TODO: Calculez les responsabilités\n    # 1. Pour chaque composant k:\n    #    weighted_pdf[:, k] = weights[k] * gaussian_pdf(X, means[k], covariances[k])\n    # 2. Normalisez chaque ligne:\n    #    responsibilities = weighted_pdf / weighted_pdf.sum(axis=1, keepdims=True)\n    # ============================================\n\n    responsibilities = None  # <- Remplacez par votre code\n\n    return responsibilities",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Test avec les vrais paramètres\ntest_pdf = gaussian_pdf(X_gmm[:2], np.array([0, 0]), np.eye(2))\n\nif test_pdf is not None:\n    r_test = e_step(X_gmm, np.array(true_weights), true_means, true_covs)\n\n    if r_test is not None:\n        print(f\"Forme des responsabilités: {r_test.shape}\")\n        print(f\"Somme par ligne (5 premiers, attendu: 1.0): {r_test[:5].sum(axis=1).round(4)}\")\n        print(f\"\\nResponsabilités (5 premiers points):\")\n        print(r_test[:5].round(3))\n\n        if np.allclose(r_test.sum(axis=1), 1.0):\n            print(\"\\nCorrect! Les responsabilités somment à 1 pour chaque point.\")\n    else:\n        print(\"Complétez la fonction e_step!\")\nelse:\n    print(\"Complétez d'abord gaussian_pdf!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary><b>Solution</b> (cliquez pour afficher)</summary>\n\n```python\ndef e_step(X, weights, means, covariances):\n    N = X.shape[0]\n    K = len(weights)\n\n    weighted_pdf = np.zeros((N, K))\n    for k in range(K):\n        weighted_pdf[:, k] = weights[k] * gaussian_pdf(X, means[k], covariances[k])\n\n    responsibilities = weighted_pdf / weighted_pdf.sum(axis=1, keepdims=True)\n    return responsibilities\n```\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercice 7: Mettre à jour les paramètres (étape M) ★★\n\nL'**étape M** met à jour les paramètres en utilisant les responsabilités comme pondérations. Soit $N_k = \\sum_{n=1}^N r_{nk}$ le nombre effectif de points dans le composant $k$:\n\n$$\\pi_k = \\frac{N_k}{N}, \\qquad \\boldsymbol{\\mu}_k = \\frac{1}{N_k} \\sum_{n=1}^N r_{nk} \\mathbf{x}_n$$\n\n$$\\boldsymbol{\\Sigma}_k = \\frac{1}{N_k} \\sum_{n=1}^N r_{nk} (\\mathbf{x}_n - \\boldsymbol{\\mu}_k)(\\mathbf{x}_n - \\boldsymbol{\\mu}_k)^\\top$$\n\nCes formules sont des versions **pondérées** des estimateurs classiques."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def m_step(X, responsibilities):\n    \"\"\"\n    Étape M: met à jour les paramètres du GMM.\n\n    Args:\n        X: données (N, D)\n        responsibilities: matrice (N, K) des responsabilités\n\n    Returns:\n        weights: nouveaux poids (K,)\n        means: nouvelles moyennes (liste de K arrays)\n        covariances: nouvelles covariances (liste de K matrices)\n    \"\"\"\n    N, D = X.shape\n    K = responsibilities.shape[1]\n\n    # ============================================\n    # TODO: Mettez à jour les paramètres\n    # 1. N_k = responsibilities.sum(axis=0)             (K,)\n    # 2. weights = N_k / N\n    # 3. Pour chaque k:\n    #    mu_k = (responsibilities[:, k:k+1] * X).sum(axis=0) / N_k[k]\n    #    diff = X - mu_k\n    #    Sigma_k = (responsibilities[:, k:k+1] * diff).T @ diff / N_k[k]\n    #    Sigma_k += 1e-6 * np.eye(D)  (régularisation)\n    # ============================================\n\n    N_k = None  # <- Nombre effectif de points par composant\n    weights = None  # <- Poids du mélange\n    means = []\n    covariances = []\n\n    for k in range(K):\n        mu_k = None  # <- Moyenne du composant k\n        means.append(mu_k)\n\n        Sigma_k = None  # <- Covariance du composant k\n        covariances.append(Sigma_k)\n\n    return weights, means, covariances",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Test de l'étape M\nif 'r_test' in dir() and r_test is not None:\n    w_new, m_new, c_new = m_step(X_gmm, r_test)\n\n    if w_new is not None and m_new[0] is not None:\n        print(\"Paramètres mis à jour (avec les vraies responsabilités):\")\n        for k in range(3):\n            print(f\"\\n  Composant {k+1}:\")\n            print(f\"    Poids: {w_new[k]:.3f} (vrai: {true_weights[k]:.3f})\")\n            print(f\"    Moyenne: [{m_new[k][0]:.2f}, {m_new[k][1]:.2f}] \"\n                  f\"(vrai: [{true_means[k][0]:.1f}, {true_means[k][1]:.1f}])\")\n\n        if np.isclose(sum(w_new), 1.0):\n            print(\"\\nCorrect! Les poids somment à 1.\")\n    else:\n        print(\"Complétez la fonction m_step!\")\nelse:\n    print(\"Complétez d'abord e_step!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary><b>Solution</b> (cliquez pour afficher)</summary>\n\n```python\ndef m_step(X, responsibilities):\n    N, D = X.shape\n    K = responsibilities.shape[1]\n\n    N_k = responsibilities.sum(axis=0)\n    weights = N_k / N\n    means = []\n    covariances = []\n\n    for k in range(K):\n        mu_k = (responsibilities[:, k:k+1] * X).sum(axis=0) / N_k[k]\n        means.append(mu_k)\n\n        diff = X - mu_k\n        Sigma_k = (responsibilities[:, k:k+1] * diff).T @ diff / N_k[k]\n        Sigma_k += 1e-6 * np.eye(D)\n        covariances.append(Sigma_k)\n\n    return weights, means, covariances\n```\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercice 8: Implémenter l'algorithme EM complet ★★\n\nL'algorithme EM alterne les étapes E et M jusqu'à convergence. Nous mesurons la convergence par la **log-vraisemblance**:\n\n$$\\log p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) = \\sum_{n=1}^N \\log \\left( \\sum_{k=1}^K \\pi_k \\, \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\right)$$\n\nLa fonction `compute_log_likelihood` est fournie. Complétez la boucle EM."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def compute_log_likelihood(X, weights, means, covariances):\n    \"\"\"Calcule la log-vraisemblance du modèle.\"\"\"\n    N = X.shape[0]\n    K = len(weights)\n\n    weighted_pdfs = np.zeros((N, K))\n    for k in range(K):\n        weighted_pdfs[:, k] = weights[k] * gaussian_pdf(X, means[k], covariances[k])\n\n    return np.sum(np.log(weighted_pdfs.sum(axis=1)))\n\n\ndef em_algorithm(X, K, n_iterations=50, seed=0):\n    \"\"\"\n    Algorithme EM pour un modèle de mélange gaussien.\n\n    Args:\n        X: données (N, D)\n        K: nombre de composants\n        n_iterations: nombre maximal d'itérations\n        seed: graine aléatoire pour l'initialisation\n\n    Returns:\n        weights, means, covariances: paramètres appris\n        responsibilities: responsabilités finales (N, K)\n        log_likelihoods: historique de la log-vraisemblance\n    \"\"\"\n    N, D = X.shape\n    np.random.seed(seed)\n\n    # Initialisation: K points aléatoires comme moyennes\n    indices = np.random.choice(N, K, replace=False)\n    means = [X[idx].copy() for idx in indices]\n    covariances = [np.eye(D) for _ in range(K)]\n    weights = np.ones(K) / K\n\n    log_likelihoods = []\n\n    # ============================================\n    # TODO: Implémentez la boucle EM\n    # Pour chaque itération:\n    # 1. responsibilities = e_step(X, weights, means, covariances)\n    # 2. weights, means, covariances = m_step(X, responsibilities)\n    # 3. ll = compute_log_likelihood(X, weights, means, covariances)\n    # 4. log_likelihoods.append(ll)\n    # ============================================\n\n    for t in range(n_iterations):\n        # Étape E\n        responsibilities = None  # <- Complétez\n\n        # Étape M\n        # <- Complétez (mettez à jour weights, means, covariances)\n        pass\n\n        # Log-vraisemblance\n        ll = None  # <- Complétez\n        log_likelihoods.append(ll)\n\n    return weights, means, covariances, responsibilities, log_likelihoods",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Exécuter l'algorithme EM\ntest_pdf = gaussian_pdf(X_gmm[:2], np.array([0, 0]), np.eye(2))\n\nif test_pdf is not None:\n    weights_em, means_em, covs_em, resp_em, ll_history = em_algorithm(\n        X_gmm, K=3, n_iterations=50, seed=42\n    )\n\n    if ll_history[0] is not None:\n        print(\"Paramètres appris par EM:\")\n        for k in range(3):\n            print(f\"\\n  Composant {k+1}:\")\n            print(f\"    Poids: {weights_em[k]:.3f}\")\n            print(f\"    Moyenne: [{means_em[k][0]:.2f}, {means_em[k][1]:.2f}]\")\n\n        print(f\"\\nLog-vraisemblance initiale: {ll_history[0]:.1f}\")\n        print(f\"Log-vraisemblance finale:   {ll_history[-1]:.1f}\")\n    else:\n        print(\"Complétez la fonction em_algorithm!\")\nelse:\n    print(\"Complétez d'abord gaussian_pdf!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary><b>Solution</b> (cliquez pour afficher)</summary>\n\n```python\ndef em_algorithm(X, K, n_iterations=50, seed=0):\n    N, D = X.shape\n    np.random.seed(seed)\n\n    indices = np.random.choice(N, K, replace=False)\n    means = [X[idx].copy() for idx in indices]\n    covariances = [np.eye(D) for _ in range(K)]\n    weights = np.ones(K) / K\n\n    log_likelihoods = []\n\n    for t in range(n_iterations):\n        responsibilities = e_step(X, weights, means, covariances)\n        weights, means, covariances = m_step(X, responsibilities)\n        ll = compute_log_likelihood(X, weights, means, covariances)\n        log_likelihoods.append(ll)\n\n    return weights, means, covariances, responsibilities, log_likelihoods\n```\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Visualiser la convergence"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "if 'll_history' in dir() and len(ll_history) > 0 and ll_history[0] is not None:\n    from matplotlib.patches import Ellipse\n\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n    # Log-vraisemblance\n    ax = axes[0]\n    ax.plot(ll_history, 'C0-', linewidth=2)\n    ax.set_xlabel('Itération')\n    ax.set_ylabel('Log-vraisemblance')\n    ax.set_title(\"Convergence de l'algorithme EM\")\n    ax.grid(True, alpha=0.3)\n\n    # Résultat du partitionnement\n    ax = axes[1]\n    z_em = np.argmax(resp_em, axis=1)\n    colors_em = ['steelblue', 'coral', 'seagreen']\n\n    for k in range(3):\n        mask = z_em == k\n        ax.scatter(X_gmm[mask, 0], X_gmm[mask, 1], c=colors_em[k], alpha=0.4,\n                   s=15, label=f'Composant {k+1} ($\\pi$={weights_em[k]:.2f})')\n\n        # Dessiner les ellipses\n        eigenvalues, eigenvectors = np.linalg.eigh(covs_em[k])\n        angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))\n        for n_std in [1, 2]:\n            width = 2 * n_std * np.sqrt(eigenvalues[0])\n            height = 2 * n_std * np.sqrt(eigenvalues[1])\n            ellipse = Ellipse(means_em[k], width, height, angle=angle, fill=False,\n                             color=colors_em[k], linewidth=2, linestyle='--')\n            ax.add_patch(ellipse)\n\n    ax.set_xlabel('$x_1$')\n    ax.set_ylabel('$x_2$')\n    ax.set_title(\"Résultat de l'algorithme EM\")\n    ax.legend()\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Complétez d'abord l'algorithme EM!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Partie 4: Comparaison GMM et k-moyennes\n\nL'algorithme k-moyennes est un cas limite de GMM où les covariances sont sphériques et identiques, et les responsabilités deviennent binaires. Comparons les deux approches sur des données avec des groupes de **formes elliptiques différentes**."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercice 9: GMM vs k-moyennes ★★★\n\nExécutez la cellule ci-dessous, puis répondez aux questions de réflexion."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from sklearn.cluster import KMeans\nfrom matplotlib.patches import Ellipse\n\nif 'll_history' in dir() and len(ll_history) > 0 and ll_history[0] is not None:\n    # Données avec des ellipses allongées\n    np.random.seed(123)\n    X_ellipse = np.vstack([\n        np.random.multivariate_normal([0, 0], [[4, 0], [0, 0.2]], 150),\n        np.random.multivariate_normal([0, 3], [[0.2, 0], [0, 4]], 150),\n    ])\n\n    # K-moyennes\n    kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n    z_kmeans = kmeans.fit_predict(X_ellipse)\n\n    # GMM (notre implémentation)\n    w_cmp, m_cmp, c_cmp, r_cmp, _ = em_algorithm(X_ellipse, K=2, n_iterations=50, seed=42)\n    z_gmm = np.argmax(r_cmp, axis=1)\n\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n    # K-moyennes\n    ax = axes[0]\n    for k in range(2):\n        mask = z_kmeans == k\n        ax.scatter(X_ellipse[mask, 0], X_ellipse[mask, 1], c=f'C{k}', alpha=0.5, s=15)\n    ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n               c='black', marker='X', s=200, zorder=5, label='Centroïdes')\n    ax.set_xlabel('$x_1$')\n    ax.set_ylabel('$x_2$')\n    ax.set_title('K-moyennes')\n    ax.legend()\n\n    # GMM\n    ax = axes[1]\n    for k in range(2):\n        mask = z_gmm == k\n        ax.scatter(X_ellipse[mask, 0], X_ellipse[mask, 1], c=f'C{k}', alpha=0.5, s=15)\n\n    for k in range(2):\n        eigenvalues, eigenvectors = np.linalg.eigh(c_cmp[k])\n        angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))\n        for n_std in [1, 2]:\n            width = 2 * n_std * np.sqrt(np.maximum(eigenvalues[0], 1e-6))\n            height = 2 * n_std * np.sqrt(np.maximum(eigenvalues[1], 1e-6))\n            ellipse = Ellipse(m_cmp[k], width, height, angle=angle, fill=False,\n                             color=f'C{k}', linewidth=2, linestyle='--')\n            ax.add_patch(ellipse)\n\n    ax.set_xlabel('$x_1$')\n    ax.set_ylabel('$x_2$')\n    ax.set_title('GMM (algorithme EM)')\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Complétez d'abord l'algorithme EM!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Questions de réflexion:**\n1. K-moyennes découpe l'espace en régions délimitées par des frontières linéaires (diagramme de Voronoï). Pourquoi cela pose-t-il problème pour des groupes allongés?\n2. GMM modélise chaque composant avec sa propre matrice de covariance. Quel avantage cela donne-t-il sur des données à formes variées?\n3. Les responsabilités souples du GMM indiquent l'incertitude d'assignation. Dans quelles applications pratiques cette information serait-elle utile?"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Récapitulatif\n\nDans ce TP, vous avez implémenté deux modèles génératifs:\n\n1. **Naïf bayésien** pour la classification de texte:\n   - Estimation des a priori $\\hat{\\pi}_c = N_c / N$ et des probabilités conditionnelles $\\hat{\\theta}_{dc}$ par maximum de vraisemblance\n   - Lissage de Laplace pour éviter les probabilités nulles\n   - Prédiction en espace logarithmique: $\\hat{y} = \\arg\\max_c [\\log \\pi_c + \\sum_d x_d \\log \\theta_{dc}]$\n\n2. **Modèle de mélange gaussien** pour le partitionnement non supervisé:\n   - Densité gaussienne multivariée et distance de Mahalanobis\n   - Responsabilités: probabilités d'appartenance souples $r_{nk} = p(z_n = k \\mid \\mathbf{x}_n)$\n   - Algorithme EM: alternance étape E (responsabilités) et étape M (paramètres pondérés)\n\nLe naïf bayésien illustre l'approche générative en classification supervisée: modéliser $p(\\mathbf{x} \\mid y)$ pour chaque classe plutôt que $p(y \\mid \\mathbf{x})$ directement. Le GMM étend cette idée au cas non supervisé, où les «classes» (composants) sont inconnues et doivent être inférées par l'algorithme EM.\n\n---\n\n**Pour aller plus loin**: [Chapitre 6: Modèles probabilistes génératifs](https://pierrelux.github.io/mlbook/ch6_probabilistic_models)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}